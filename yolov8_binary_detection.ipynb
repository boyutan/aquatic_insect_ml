{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0200c5e2",
      "metadata": {
        "id": "0200c5e2"
      },
      "source": [
        "\n",
        "# YOLOv8 Binary Object Detection and Cropping Pipeline\n",
        "\n",
        "This notebook includes:\n",
        "\n",
        "1. Converting YOLO dataset annotations from multi-class to binary.\n",
        "2. Fine-tuning YOLOv8 for binary detection.\n",
        "3. Inference pipeline (detection -> cropping -> augmentation -> classification preparation).\n",
        "\n",
        "Abdulmlik almuhanna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6ce577f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ce577f9",
        "outputId": "a8a38aba-8be8-4efb-9202-7417ed9b3362"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q ultralytics roboflow\n",
        "!pip install -q glob2\n",
        "import os\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c53c1cec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c53c1cec",
        "outputId": "116a6af7-f38e-4d63-9357-394093e7032b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"TS1niacLXvvWTierCKCT\")\n",
        "project = rf.workspace(\"insectai\").project(\"insects_noclasses\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EzzX8udV6pt7",
      "metadata": {
        "id": "EzzX8udV6pt7"
      },
      "source": [
        "### create negative images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fj4wWsE26pXt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj4wWsE26pXt",
        "outputId": "02004a18-48c4-4dc9-a490-26346f5b00fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Done! Inpainted images saved to: negative_images_inpainted\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# === Paths ===\n",
        "image_folder = '/content/insects_noclasses-7/train/images'\n",
        "label_folder = '/content/insects_noclasses-7/train/labels'\n",
        "output_folder = 'negative_images_inpainted'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    x_center, y_center, w, h = yolo_bbox\n",
        "    x_center *= img_width\n",
        "    y_center *= img_height\n",
        "    w *= img_width\n",
        "    h *= img_height\n",
        "    x1 = int(x_center - w / 2)\n",
        "    y1 = int(y_center - h / 2)\n",
        "    x2 = int(x_center + w / 2)\n",
        "    y2 = int(y_center + h / 2)\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + '.txt')\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "        # Create mask from bbox annotations\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) != 5:\n",
        "                        continue\n",
        "                    _, *bbox = map(float, parts)\n",
        "                    x1, y1, x2, y2 = yolo_to_bbox(bbox, w, h)\n",
        "                    cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)  # White mask\n",
        "\n",
        "            # Inpaint using nearby pixels\n",
        "            inpainted = cv2.inpaint(img, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
        "            cv2.imwrite(output_path, inpainted)\n",
        "\n",
        "print(\"✅ Done! Inpainted images saved to:\", output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BNec_p0QCLuH",
      "metadata": {
        "id": "BNec_p0QCLuH"
      },
      "source": [
        "### Load Negative images to our Train folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "vrTqilKiFL7_",
      "metadata": {
        "id": "vrTqilKiFL7_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘negs’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir negs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "B_yHhWUeCSFQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_yHhWUeCSFQ",
        "outputId": "6dcc476a-e54f-4ceb-b5a0-72a5654a7b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 21 negative images copied to: /hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/train/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# === INPUT CONFIG ===\n",
        "source_folder = '/hpc/home/ama191/Our_Yolo_Solution/negs'   # Folder with negative images\n",
        "target_folder = '/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/train/images'        # Your YOLO training images folder\n",
        "\n",
        "# === Ensure target folder exists ===\n",
        "os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "# === Loop through and copy only image files ===\n",
        "image_extensions = ('.jpg', '.jpeg', '.png')\n",
        "\n",
        "for filename in os.listdir(source_folder):\n",
        "    if filename.lower().endswith(image_extensions):\n",
        "        src_path = os.path.join(source_folder, filename)\n",
        "        dst_path = os.path.join(target_folder, filename)\n",
        "        shutil.copy2(src_path, dst_path)  # use shutil.move if you want to move instead of copy\n",
        "\n",
        "print(f\"✅ {len(os.listdir(source_folder))} negative images copied to: {target_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EOt_wW1RchTa",
      "metadata": {
        "id": "EOt_wW1RchTa"
      },
      "source": [
        "### this is for removing classes but we dont need it now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0818cf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0818cf1",
        "outputId": "41343d74-97ac-43fb-ba63-fbe8954e8070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted labels to binary classes successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import glob\n",
        "\n",
        "label_paths = glob.glob(f\"{dataset.location}/train/labels/*.txt\")\n",
        "\n",
        "for label_path in label_paths:\n",
        "    with open(label_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    with open(label_path, \"w\") as file:\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            parts[0] = '0'  # Binary class (0: insect)\n",
        "            file.write(' '.join(parts) + '\\n')\n",
        "\n",
        "label_paths2 = glob.glob(f\"{dataset.location}/valid/labels/*.txt\")\n",
        "\n",
        "for label_path in label_paths2:\n",
        "    with open(label_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    with open(label_path, \"w\") as file:\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            parts[0] = '0'  # Binary class (0: insect)\n",
        "            file.write(' '.join(parts) + '\\n')\n",
        "print(\"Converted labels to binary classes successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ed10da",
      "metadata": {
        "id": "b5ed10da",
        "outputId": "c5f573f1-9872-4de4-cdc8-c66f8f330570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.7.0)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.local/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: triton==3.3.0 in ./.local/lib/python3.9/site-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.local/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.local/lib/python3.9/site-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.local/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.local/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.local/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.local/lib/python3.9/site-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.local/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.9/site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.local/lib/python3.9/site-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.local/lib/python3.9/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.local/lib/python3.9/site-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.local/lib/python3.9/site-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.9/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3.9/site-packages (from triton==3.3.0->torch) (53.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib64/python3.9/site-packages (from jinja2->torch) (1.1.1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "498"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41541843",
      "metadata": {},
      "source": [
        "### creating a pose Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "64121030",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "labels = glob(\"/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/valid/labels/*.txt\", recursive=True)\n",
        "\n",
        "root_dir = Path(labels[0]).parent.parent.parent\n",
        "for label in labels:\n",
        "    label = Path(label)\n",
        "    split = label.parent.parent.name\n",
        "    img_path = [(root_dir / split / \"images\" / label.with_suffix(sfx).name)\n",
        "                for sfx in [\".png\", \".jpg\", \".PNG\", \".JPG\"]]\n",
        "    img_path = [pth for pth in img_path if pth.exists()][0]\n",
        "    boxes = []\n",
        "    points = []\n",
        "    classes = []\n",
        "    save_pth = root_dir / split / \"labels_kp\" / label.name\n",
        "    save_pth.parent.mkdir(exist_ok=True)\n",
        "    with open(label) as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            splits = line.rstrip().split(\" \")\n",
        "            cls_id = int(splits[0])\n",
        "            box = splits[1:]\n",
        "            if not box:\n",
        "                with open(save_pth, \"w\") as f:\n",
        "                    pass\n",
        "                continue\n",
        "\n",
        "            box = [float(pt) for pt in box]\n",
        "            point = (box[0], box[1])\n",
        "            points.append(point)\n",
        "            boxes.append(box)\n",
        "            classes.append(cls_id)\n",
        "\n",
        "    with open(save_pth, \"w\") as f:\n",
        "        for point, box, cls_id in zip(points, boxes, classes):\n",
        "            f.writelines(f\"{cls_id} {box[0]} {box[1]} {box[2]} {box[3]} {point[0]} {point[1]} 1 \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tkYSeYXncqnU",
      "metadata": {
        "id": "tkYSeYXncqnU"
      },
      "source": [
        "### training the binary detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rdsIjnGRTdSd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdsIjnGRTdSd",
        "outputId": "84ec02cf-3494-4212-c9e2-9757869d7252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo11x-p2-coco'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (11/11), 3.62 KiB | 21.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/davsolai/yolo11x-p2-coco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "qvcBK4XRTj5p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvcBK4XRTj5p",
        "outputId": "69f26e8e-6f5b-4aaa-b1d8-ce25d031af80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From https://huggingface.co/davsolai/yolo11x-p2-coco\n",
            " * [new ref]         refs/pr/1  -> pr/1\n"
          ]
        }
      ],
      "source": [
        "!cd yolo11x-p2-coco && git fetch origin refs/pr/1:pr/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893dce77",
      "metadata": {},
      "outputs": [],
      "source": [
        "!yolo cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c7bdb2ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=runs/detect/tune2'\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m💡 Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/300 with hyperparameters: {'lr0': 0.01, 'degrees': 0.0, 'dfl': 1.5}\n",
            "New https://pypi.org/project/ultralytics/8.3.157 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml, degrees=0.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/hpc/home/ama191/Our_Yolo_Solution/yolo11x-p2-coco/model (1).pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
            "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
            "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
            "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
            " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
            " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 18             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  2    426240  ultralytics.nn.modules.block.C3k2            [768, 192, 2, True]           \n",
            " 20                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 21            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  2   1331712  ultralytics.nn.modules.block.C3k2            [576, 384, 2, True]           \n",
            " 23                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 24            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 25                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
            " 26                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            " 27            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 28                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 29    [19, 22, 25, 28]  1   1970132  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768, 768]]     \n",
            "YOLO11x-p2 summary: 440 layers, 57,788,468 parameters, 57,788,452 gradients, 244.1 GFLOPs\n",
            "\n",
            "Transferred 1245/1253 items from pretrained weights\n",
            "\u001b[34m\u001b[1mDDP:\u001b[0m debug command /hpc/home/ama191/miniconda3/envs/test/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 49307 /hpc/home/ama191/.config/Ultralytics/DDP/_temp_u672v8fg139792836792080.py\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "                                                        CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/hpc/home/ama191/.config/Ultralytics/DDP/_temp_u672v8fg139792836792080.py\", line 13, in <module>\n",
            "    results = trainer.train()\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 227, in train\n",
            "    self._do_train(world_size)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 347, in _do_train\n",
            "    self._setup_ddp(world_size)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 239, in _setup_ddp\n",
            "    torch.cuda.set_device(RANK)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 529, in set_device\n",
            "    torch._C._cuda_setDevice(device)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0620 14:51:45.401000 2015958 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2015979 closing signal SIGTERM\n",
            "E0620 14:51:45.475000 2015958 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 2015980) of binary: /hpc/home/ama191/miniconda3/envs/test/bin/python\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 896, in <module>\n",
            "    main()\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 892, in main\n",
            "    run(args)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 883, in run\n",
            "    elastic_launch(\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/hpc/home/ama191/.config/Ultralytics/DDP/_temp_u672v8fg139792836792080.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-06-20_14:51:45\n",
            "  host      : dcc-courses-gpu-01.rc.duke.edu\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 2015980)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 1025, in <module>\n",
            "    entrypoint(debug=\"\")\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 983, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 797, in train\n",
            "    self.trainer.train()\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 222, in train\n",
            "    raise e\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 220, in train\n",
            "    subprocess.run(cmd, check=True)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/subprocess.py\", line 571, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['/hpc/home/ama191/miniconda3/envs/test/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '49307', '/hpc/home/ama191/.config/Ultralytics/DDP/_temp_u672v8fg139792836792080.py']' returned non-zero exit status 1.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR ❌ training failure for hyperparameter tuning iteration 1\n",
            "Command '['/hpc/home/ama191/miniconda3/envs/test/bin/python', '-m', 'ultralytics.cfg.__init__', 'train', 'task=detect', 'mode=train', 'model=/hpc/home/ama191/Our_Yolo_Solution/yolo11x-p2-coco/model (1).pt', 'data=/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml', 'epochs=30', 'time=None', 'patience=100', 'batch=16', 'imgsz=1024', 'save=True', 'save_period=-1', 'cache=False', 'device=None', 'workers=8', 'project=None', 'name=None', 'exist_ok=False', 'pretrained=True', 'optimizer=AdamW', 'verbose=True', 'seed=0', 'deterministic=True', 'single_cls=False', 'rect=False', 'cos_lr=False', 'close_mosaic=10', 'resume=False', 'amp=True', 'fraction=1.0', 'profile=False', 'freeze=None', 'multi_scale=False', 'overlap_mask=True', 'mask_ratio=4', 'dropout=0.0', 'val=True', 'split=val', 'save_json=False', 'conf=None', 'iou=0.7', 'max_det=300', 'half=False', 'dnn=False', 'plots=True', 'source=None', 'vid_stride=1', 'stream_buffer=False', 'visualize=False', 'augment=False', 'agnostic_nms=False', 'classes=None', 'retina_masks=False', 'embed=None', 'show=False', 'save_frames=False', 'save_txt=False', 'save_conf=False', 'save_crop=False', 'show_labels=True', 'show_conf=True', 'show_boxes=True', 'line_width=None', 'format=torchscript', 'keras=False', 'optimize=False', 'int8=False', 'dynamic=False', 'simplify=True', 'opset=None', 'workspace=None', 'nms=False', 'lr0=0.01', 'lrf=0.01', 'momentum=0.937', 'weight_decay=0.0005', 'warmup_epochs=3.0', 'warmup_momentum=0.8', 'warmup_bias_lr=0.1', 'box=7.5', 'cls=0.5', 'dfl=1.5', 'pose=12.0', 'kobj=1.0', 'nbs=64', 'hsv_h=0.015', 'hsv_s=0.7', 'hsv_v=0.4', 'degrees=0.0', 'translate=0.1', 'scale=0.5', 'shear=0.0', 'perspective=0.0', 'flipud=0.0', 'fliplr=0.5', 'bgr=0.0', 'mosaic=1.0', 'mixup=0.0', 'cutmix=0.0', 'copy_paste=0.0', 'copy_paste_mode=flip', 'auto_augment=randaugment', 'erasing=0.4', 'cfg=None', 'tracker=botsort.yaml']' returned non-zero exit status 1.\n",
            "Saved runs/detect/tune2/tune_scatter_plots.png\n",
            "Saved runs/detect/tune2/tune_fitness.png\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0m1/300 iterations complete ✅ (26.90s)\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns/detect/tune2\u001b[0m\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.0 observed at iteration 1\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {}\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs/detect/train2\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
            "\n",
            "Printing '\u001b[1m\u001b[30mruns/detect/tune2/best_hyperparameters.yaml\u001b[0m'\n",
            "\n",
            "lr0: 0.01\n",
            "degrees: 0.0\n",
            "dfl: 1.5\n",
            "\n",
            "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 2/300 with hyperparameters: {'lr0': 0.00912, 'degrees': 0.0, 'dfl': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/30      13.9G      3.708      4.855      2.067       4537        640:  14%|█▍        | 1/7 [00:43<04:22, 43.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\n",
            "New https://pypi.org/project/ultralytics/8.3.157 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml, degrees=0.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.00912, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/hpc/home/ama191/Our_Yolo_Solution/yolo11x-p2-coco/model (1).pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
            "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
            "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
            "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
            " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
            " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 18             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  2    426240  ultralytics.nn.modules.block.C3k2            [768, 192, 2, True]           \n",
            " 20                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 21            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  2   1331712  ultralytics.nn.modules.block.C3k2            [576, 384, 2, True]           \n",
            " 23                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 24            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 25                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
            " 26                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            " 27            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 28                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 29    [19, 22, 25, 28]  1   1970132  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768, 768]]     \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0620 14:51:58.848000 2015573 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2015588 closing signal SIGTERM\n",
            "E0620 14:51:59.271000 2015573 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -9) local_rank: 0 (pid: 2015587) of binary: /hpc/home/ama191/miniconda3/envs/test/bin/python\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 896, in <module>\n",
            "    main()\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 892, in main\n",
            "    run(args)\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/run.py\", line 883, in run\n",
            "    elastic_launch(\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/hpc/home/ama191/miniconda3/envs/test/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/hpc/home/ama191/.config/Ultralytics/DDP/_temp_uteokor4139946205078736.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-06-20_14:51:58\n",
            "  host      : dcc-courses-gpu-01.rc.duke.edu\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : -9 (pid: 2015587)\n",
            "  error_file: <N/A>\n",
            "  traceback : Signal 9 (SIGKILL) received by PID 2015587\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11x-p2 summary: 440 layers, 57,788,468 parameters, 57,788,452 gradients, 244.1 GFLOPs\n",
            "\n",
            "Transferred 1245/1253 items from pretrained weights\n",
            "\u001b[34m\u001b[1mDDP:\u001b[0m debug command /hpc/home/ama191/miniconda3/envs/test/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 34653 /hpc/home/ama191/.config/Ultralytics/DDP/_temp_y5z3kgcl140233061802320.py\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m search_space = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr0\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-5\u001b[39m, \u001b[32m1e-1\u001b[39m),\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdegrees\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.0\u001b[39m, \u001b[32m45.0\u001b[39m),\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdfl\u001b[39m\u001b[33m\"\u001b[39m:(\u001b[32m0.5\u001b[39m,\u001b[32m7\u001b[39m)\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Tune hyperparameters on COCO8 for 30 epochs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model.tune(\n\u001b[32m     15\u001b[39m     data=\u001b[33m\"\u001b[39m\u001b[33m/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     epochs=\u001b[32m30\u001b[39m,\n\u001b[32m     17\u001b[39m     iterations=\u001b[32m300\u001b[39m,\n\u001b[32m     18\u001b[39m     optimizer=\u001b[33m\"\u001b[39m\u001b[33mAdamW\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     space=search_space,\n\u001b[32m     20\u001b[39m     plots=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m     save=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     22\u001b[39m     val=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     23\u001b[39m     imgsz=\u001b[32m1024\u001b[39m\n\u001b[32m     24\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/model.py:852\u001b[39m, in \u001b[36mModel.tune\u001b[39m\u001b[34m(self, use_ray, iterations, *args, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m custom = {}  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[32m    851\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Tuner(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)(model=\u001b[38;5;28mself\u001b[39m, iterations=iterations)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/site-packages/ultralytics/engine/tuner.py:199\u001b[39m, in \u001b[36mTuner.__call__\u001b[39m\u001b[34m(self, model, iterations, cleanup)\u001b[39m\n\u001b[32m    197\u001b[39m launch = [\u001b[38;5;28m__import__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msys\u001b[39m\u001b[33m\"\u001b[39m).executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33multralytics.cfg.__init__\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# workaround yolo not found\u001b[39;00m\n\u001b[32m    198\u001b[39m cmd = [*launch, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, *(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_args.items())]\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m return_code = subprocess.run(cmd, check=\u001b[38;5;28;01mTrue\u001b[39;00m).returncode\n\u001b[32m    200\u001b[39m ckpt_file = weights_dir / (\u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (weights_dir / \u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlast.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m metrics = torch.load(ckpt_file)[\u001b[33m\"\u001b[39m\u001b[33mtrain_metrics\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28mself\u001b[39m.wait()\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait(timeout=timeout)\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28mself\u001b[39m._try_wait(\u001b[32m0\u001b[39m)\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/test/lib/python3.11/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = os.waitpid(\u001b[38;5;28mself\u001b[39m.pid, wait_flags)\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Initialize the YOLO model\n",
        "model = YOLO(\"/hpc/home/ama191/Our_Yolo_Solution/yolo11x-p2-coco/model (1).pt\")\n",
        "\n",
        "# Define search space\n",
        "search_space = {\n",
        "    \"lr0\": (1e-5, 1e-1),\n",
        "    \"degrees\": (0.0, 45.0),\n",
        "    \"dfl\":(0.5,7)\n",
        "}\n",
        "\n",
        "# Tune hyperparameters on COCO8 for 30 epochs\n",
        "model.tune(\n",
        "    data=\"/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml\",\n",
        "    epochs=30,\n",
        "    iterations=300,\n",
        "    optimizer=\"AdamW\",\n",
        "    space=search_space,\n",
        "    plots=True,\n",
        "    save=True,\n",
        "    val=True,\n",
        "    imgsz=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Vrp86wlFg5WK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrp86wlFg5WK",
        "outputId": "4450984f-4a02-4018-87b0-b6cd895573ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters saved to 'cgs.yaml'.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "# Define hyperparameters for the detector model\n",
        "hyperparams = { # Replace this dynamically if needed\n",
        "    'epochs': 50,\n",
        "    'imgsz': 800,\n",
        "    'batch': 2,\n",
        "    'project': 'binary_detector',\n",
        "    'max_det': 600,\n",
        "    'dfl':3.5,\n",
        "    'cls':0.8\n",
        "\n",
        "}\n",
        "\n",
        "# Save the hyperparameters to a YAML file\n",
        "with open(\"cgs.yaml\", \"w\") as file:\n",
        "  yaml.dump(hyperparams, file)\n",
        "print(\"Hyperparameters saved to 'cgs.yaml'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "107a8ade",
      "metadata": {
        "collapsed": true,
        "id": "107a8ade"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fine-tune YOLOv8 on binary dataset\n",
        "#model = YOLO(\"/hpc/home/ama191/Our_Yolo_Solution/yolo11x-p2-coco/model (1).pt\")\n",
        "model = YOLO('/hpc/home/ama191/Our_Yolo_Solution/binary_detector/train31/weights/best.pt')  # You can change modl size\n",
        "#model = YOLO(\"yolo11x-pose\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3009RJCQNs4W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3009RJCQNs4W",
        "outputId": "e3517dc5-d831-4499-f79b-8dbc18bb4993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.157 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=2, bgr=0.0, box=7.5, cache=False, cfg=/hpc/home/ama191/Our_Yolo_Solution/cgs.yaml, classes=None, close_mosaic=10, cls=0.8, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/data.yaml, degrees=0.0, deterministic=True, device=0,1, dfl=3.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=800, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=600, mixup=0.0, mode=train, model=/hpc/home/ama191/Our_Yolo_Solution/binary_detector/train31/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train32, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=binary_detector, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=binary_detector/train32, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
            "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
            "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
            "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
            " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
            " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 18             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  2    426240  ultralytics.nn.modules.block.C3k2            [768, 192, 2, True]           \n",
            " 20                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 21            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  2   1331712  ultralytics.nn.modules.block.C3k2            [576, 384, 2, True]           \n",
            " 23                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 24            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 25                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
            " 26                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            " 27            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 28                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 29    [19, 22, 25, 28]  1   1970132  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768, 768]]     \n",
            "YOLO11x-p2 summary: 440 layers, 57,788,468 parameters, 57,788,452 gradients, 244.1 GFLOPs\n",
            "\n",
            "Transferred 1253/1253 items from pretrained weights\n",
            "\u001b[34m\u001b[1mDDP:\u001b[0m debug command /hpc/home/ama191/miniconda3/envs/test/bin/python -m torch.distributed.run --nproc_per_node 2 --master_port 49965 /hpc/home/ama191/.config/Ultralytics/DDP/_temp_tdwd_269139686415020048.py\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "                                                        CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "Transferred 1253/1253 items from pretrained weights\n",
            "Freezing layer 'model.29.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.1 ms, read: 100.3±40.1 MB/s, size: 241.9 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/train/labels.cache... 105 images, 21 backgrounds, 0 corrupt: 100%|██████████| 126/126 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.6±0.1 ms, read: 46.0±23.5 MB/s, size: 277.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/valid/labels.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to binary_detector/train32/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 206 weight(decay=0.0), 215 weight(decay=0.0005), 214 bias(decay=0.0)\n",
            "Image sizes 800 train, 800 val\n",
            "Using 16 dataloader workers\n",
            "Logging results to \u001b[1mbinary_detector/train32\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/50      9.18G      1.779      1.818      2.709         76        800: 100%|██████████| 63/63 [00:31<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.715      0.569      0.624      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/50      7.95G      1.868      4.131      2.825         82        800: 100%|██████████| 63/63 [00:30<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444       0.66      0.534      0.579      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/50      7.29G       1.87      1.942      2.858         82        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.571       0.49      0.505      0.197\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/50      7.43G      1.964      2.043      2.952         91        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.564      0.462      0.489      0.188\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/50      8.96G      1.938      2.386      2.939        199        800: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.484      0.423       0.43      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/50      6.87G      2.028      2.092      2.991        263        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.632      0.545       0.57       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/50      7.01G       1.97      3.043      2.932         18        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.502      0.485      0.475      0.181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/50      7.01G      1.946       3.37      3.155        161        800: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.414      0.406       0.35      0.124\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/50      7.01G      1.974      5.525      2.901        174        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.526      0.493      0.475      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/50      8.42G      1.961      11.46      2.939        241        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.575      0.516      0.513      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/50       6.7G      1.941      3.424      2.985        292        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.596      0.532      0.543      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/50      8.02G      2.011      4.439      2.933        183        800: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.587      0.529      0.545        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/50      6.65G      2.004      2.577      3.052         23        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.631       0.54      0.582      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/50      7.81G      1.913       2.26      2.888        548        800: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.617      0.537      0.554      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/50      7.81G      1.963       2.42      3.015         61        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.561      0.507      0.517      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/50      7.81G      1.898      2.174      2.894          6        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.526      0.468      0.458      0.171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/50      7.95G      1.951      2.098      2.967        257        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444       0.51      0.451      0.445      0.162\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/50      7.17G      1.914       2.48       3.05         47        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.619       0.53      0.571      0.217\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/50      7.32G      1.942      2.566      2.874         12        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.615      0.525      0.549      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/50      9.74G      1.983      2.642      2.969        333        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.563      0.508      0.514      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      21/50      7.17G      2.034      2.045        3.1         15        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.598       0.53      0.553      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      22/50      7.31G      1.856      2.036      2.934         12        800: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.638      0.553      0.588      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/50      7.31G      1.947      1.995       2.94        529        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.588      0.521      0.534      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/50      7.31G      1.994      2.451      3.032        170        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.559       0.48      0.489      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/50      7.46G      1.804       1.91      2.675        104        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.661       0.57      0.606      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/50       9.3G      1.975      2.154      2.911         16        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.695      0.578      0.618      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/50      6.81G      1.857      2.094      2.884        426        800: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.566      0.477      0.502      0.186\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/50      6.96G      1.953      2.115      2.984        412        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.552      0.477        0.5      0.188\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/50      6.96G      1.887       1.92      2.938         12        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.684      0.561      0.619      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/50      6.96G      1.899      2.046      2.972        494        800: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.699      0.578      0.632      0.246\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/50      8.18G      1.805      2.079      2.843         76        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.649      0.556      0.602       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/50      7.52G      1.974      1.867       2.88        287        800: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.633      0.533      0.584      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      33/50      7.66G      1.941      1.925      2.951          0        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.671      0.548      0.606      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      34/50      7.66G      1.965      1.978      3.044        475        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.721      0.572      0.635      0.247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      35/50      7.66G      1.955      1.955      2.854         90        800: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.726      0.568      0.635       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      36/50       7.8G      1.788      1.813       2.81        102        800: 100%|██████████| 63/63 [00:29<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.687      0.562      0.619      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      37/50       7.8G      1.977      1.888      2.919        229        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.681      0.567      0.621      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      38/50       7.8G      2.008      1.927      2.936         89        800: 100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.715      0.568      0.642       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      39/50      11.8G      1.913      1.836      2.763         87        800: 100%|██████████| 63/63 [00:29<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.669      0.564      0.617       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      40/50      7.48G      1.892      1.771      2.847        562        800: 100%|██████████| 63/63 [00:30<00:00,  2.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.691      0.547      0.618       0.24\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      41/50      7.62G      1.633      1.706      2.507         76        800: 100%|██████████| 63/63 [00:29<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.636      0.523      0.579      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      42/50      7.62G      1.601      1.588      2.551         76        800: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444        0.6       0.53      0.558      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      43/50      7.63G      1.395      1.385      2.304        161        800: 100%|██████████| 63/63 [00:29<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.613      0.525       0.57      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      44/50      7.77G      1.548      1.508      2.482        340        800: 100%|██████████| 63/63 [00:28<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.635      0.546      0.592      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      45/50      7.77G      1.493      1.444      2.357        135        800: 100%|██████████| 63/63 [00:29<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.639      0.556      0.604      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      46/50      7.77G      1.566      1.482      2.428         17        800: 100%|██████████| 63/63 [00:28<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444       0.66      0.571      0.623      0.252\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      47/50      7.77G      1.431      1.359       2.27        523        800: 100%|██████████| 63/63 [00:28<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.624      0.529       0.58      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      48/50      7.77G      1.446      1.374      2.336         17        800: 100%|██████████| 63/63 [00:28<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.634      0.549      0.591      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      49/50      7.77G      1.536      1.484      2.555        553        800: 100%|██████████| 63/63 [00:28<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.644      0.559      0.606       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      50/50      7.77G      1.515      1.485      2.516         17        800: 100%|██████████| 63/63 [00:28<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.688      0.535      0.611      0.245\n",
            "\n",
            "50 epochs completed in 0.455 hours.\n",
            "Optimizer stripped from binary_detector/train32/weights/last.pt, 116.5MB\n",
            "Optimizer stripped from binary_detector/train32/weights/best.pt, 116.5MB\n",
            "\n",
            "Validating binary_detector/train32/weights/best.pt...\n",
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "                                                        CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "YOLO11x-p2 summary (fused): 234 layers, 57,739,092 parameters, 0 gradients, 242.6 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.678      0.561      0.618      0.253\n",
            "Speed: 0.4ms preprocess, 494.7ms inference, 0.0ms loss, 6.0ms postprocess per image\n",
            "Results saved to \u001b[1mbinary_detector/train32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "model.train(data=f\"{dataset.location}/data.yaml\",cfg=\"/hpc/home/ama191/Our_Yolo_Solution/cgs.yaml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab1dd56",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7179fa2d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "-t9B_-IRs97b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-t9B_-IRs97b",
        "outputId": "0949d232-8230-4f56-dc50-c2893cac5e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.152 🚀 Python-3.11.11 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "                                                        CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11x-p2 summary (fused): 234 layers, 57,739,092 parameters, 0 gradients, 242.6 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.7±0.1 ms, read: 134.8±30.0 MB/s, size: 277.6 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /hpc/home/ama191/Our_Yolo_Solution/insects_noclasses-7/valid/labels.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4       2444      0.705      0.577       0.63      0.251\n",
            "Speed: 1.2ms preprocess, 642.4ms inference, 0.0ms loss, 56.9ms postprocess per image\n",
            "Results saved to \u001b[1mbinary_detector/train302\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7efcd17c00d0>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,     0.92857,     0.92857,     0.92857,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,\n",
              "            0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91304,     0.91304,     0.91304,     0.91304,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,     0.90625,\n",
              "            0.90625,     0.90625,     0.90076,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,\n",
              "            0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,\n",
              "            0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89552,     0.89474,     0.89474,     0.89474,     0.89474,     0.89474,     0.89474,     0.89003,     0.88552,     0.88552,     0.88372,     0.88158,     0.87987,     0.87859,     0.87859,     0.87838,     0.87838,\n",
              "            0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87838,     0.87766,     0.87766,     0.87737,     0.87737,     0.87737,\n",
              "            0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,\n",
              "            0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,\n",
              "            0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,\n",
              "            0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,\n",
              "            0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87737,     0.87627,     0.87446,     0.87393,     0.87393,     0.87252,     0.87252,     0.87201,\n",
              "            0.87201,     0.87115,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.87025,     0.86951,     0.86951,     0.86951,     0.86951,\n",
              "            0.86946,     0.86946,     0.86946,     0.86946,     0.86946,     0.86902,     0.86902,     0.86855,     0.86855,     0.86855,     0.86855,     0.86855,     0.86855,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,     0.86817,\n",
              "            0.86423,     0.86369,     0.86215,     0.86211,     0.86211,     0.86211,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86192,\n",
              "            0.86192,     0.86192,     0.86192,     0.86192,     0.86192,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86071,     0.86025,     0.85905,     0.85905,      0.8589,      0.8589,     0.85785,\n",
              "            0.85785,     0.85785,     0.85785,     0.85785,     0.85785,     0.85785,     0.85785,     0.85785,     0.85658,     0.85631,     0.85631,     0.85631,     0.85631,     0.85631,     0.85631,     0.85631,     0.85328,     0.85072,     0.85072,     0.85019,     0.84981,     0.84906,     0.84906,\n",
              "            0.84846,     0.84846,     0.84846,      0.8475,      0.8475,      0.8475,      0.8475,     0.84686,     0.84587,     0.84538,     0.84426,     0.84287,     0.84239,     0.84196,     0.84196,     0.84196,     0.84196,     0.84196,     0.84178,     0.84178,     0.84025,     0.84025,     0.83961,\n",
              "            0.83961,     0.83916,     0.83753,     0.83753,     0.83651,     0.83607,     0.83519,     0.83476,     0.83376,     0.83291,     0.83291,     0.83291,     0.83291,     0.83291,     0.83291,      0.8311,     0.83085,     0.83085,     0.83085,     0.83045,     0.83045,     0.83045,     0.82883,\n",
              "            0.82792,     0.82792,     0.82792,     0.82739,      0.8264,      0.8264,      0.8264,      0.8264,      0.8264,     0.82484,     0.82451,     0.82451,     0.82451,     0.82413,     0.82367,     0.82367,     0.82321,     0.82321,     0.82321,     0.82099,     0.82099,     0.82014,     0.82014,\n",
              "            0.81941,     0.81941,     0.81941,     0.81941,     0.81941,     0.81941,     0.81941,     0.81941,     0.81941,     0.81934,     0.81764,     0.81683,     0.81649,     0.81643,      0.8161,     0.81563,       0.812,     0.80991,     0.80983,     0.80983,     0.80983,     0.80836,     0.80791,\n",
              "            0.80775,     0.80528,     0.80498,     0.80479,     0.80479,     0.80479,     0.80479,      0.8045,     0.80421,     0.80392,     0.80307,     0.80262,     0.80262,     0.80262,     0.80262,     0.80262,      0.8022,      0.8022,     0.80192,     0.80109,     0.80054,     0.80054,     0.80054,\n",
              "            0.79892,     0.79852,     0.79772,      0.7972,      0.7972,      0.7972,     0.79641,     0.79509,     0.79497,      0.7942,     0.79251,     0.79227,     0.79164,     0.79036,     0.79026,     0.78876,     0.78876,     0.78815,     0.78792,     0.78745,     0.78458,     0.78299,     0.78275,\n",
              "            0.78275,     0.78275,     0.78275,     0.77938,     0.77903,     0.77861,     0.77778,     0.77716,     0.77716,     0.77682,     0.77437,     0.77384,     0.77264,     0.77264,     0.77245,     0.77226,     0.76923,     0.76923,     0.76872,     0.76854,     0.76836,     0.76694,     0.76586,\n",
              "            0.76551,     0.76533,     0.76216,      0.7614,     0.76107,     0.76088,     0.76088,     0.76058,     0.75954,     0.75704,     0.75614,     0.75614,     0.75614,     0.75513,     0.75482,     0.75411,     0.75325,      0.7531,      0.7514,     0.75084,     0.75084,     0.74903,     0.74847,\n",
              "            0.74682,     0.74601,     0.74465,      0.7433,     0.74129,     0.74116,     0.73958,     0.73906,     0.73552,     0.73515,     0.73372,     0.73065,     0.73016,      0.7256,     0.72135,     0.72029,     0.71909,      0.7162,     0.71429,     0.71348,     0.71153,     0.71038,     0.70995,\n",
              "            0.70953,     0.70748,     0.70565,     0.70433,      0.6999,      0.6997,     0.69569,      0.6943,     0.67985,     0.67886,     0.67388,     0.67261,     0.66589,     0.65829,     0.65829,     0.65547,     0.65489,     0.65388,     0.65096,      0.6468,     0.64539,     0.63672,      0.6366,\n",
              "            0.63609,     0.62676,     0.62644,     0.62146,     0.61735,     0.61445,      0.6129,     0.61135,      0.6098,     0.60825,     0.60669,     0.60514,     0.60359,     0.60204,     0.60049,     0.59894,     0.59738,     0.59583,     0.59428,     0.59273,     0.59118,     0.58963,     0.58807,\n",
              "            0.58652,     0.58497,     0.58342,     0.58187,     0.58032,     0.57876,     0.57721,     0.57566,     0.57411,     0.57256,     0.57101,     0.56945,      0.5679,     0.56635,      0.5648,     0.56325,      0.5617,     0.56014,     0.55859,     0.55704,     0.55549,     0.55394,     0.55239,\n",
              "            0.55083,     0.54928,     0.54773,     0.54618,     0.54463,     0.54308,     0.54152,     0.53997,     0.53842,     0.53687,     0.53532,     0.53377,     0.53221,     0.53066,     0.52911,     0.52756,     0.52601,     0.52446,      0.5229,     0.52135,      0.5198,     0.51825,      0.5167,\n",
              "            0.51515,     0.51359,     0.51204,     0.51049,     0.50894,     0.50739,     0.50584,     0.50429,     0.50273,     0.50118,     0.49963,     0.49808,     0.49653,     0.49498,     0.49342,     0.49187,     0.49032,     0.48877,     0.48722,     0.48567,     0.48411,     0.48256,     0.48101,\n",
              "            0.47946,     0.47791,     0.47636,      0.4748,     0.47325,      0.4717,     0.47015,      0.4686,     0.46705,     0.46549,     0.46394,     0.46239,     0.46084,     0.45929,     0.45774,     0.45618,     0.45463,     0.45308,     0.45153,     0.44998,     0.44843,     0.44687,     0.44532,\n",
              "            0.44377,     0.44222,     0.44067,     0.43912,     0.43756,     0.43601,     0.43446,     0.43291,     0.43136,     0.42981,     0.42825,      0.4267,     0.42515,      0.4236,     0.42205,      0.4205,     0.41894,     0.41739,     0.41584,     0.41429,     0.41274,     0.41119,     0.40963,\n",
              "            0.40808,     0.40653,     0.40498,     0.40343,     0.40188,     0.40032,     0.39877,     0.39722,     0.39567,     0.39412,     0.39257,     0.39101,     0.38946,     0.38791,     0.38636,     0.38481,     0.38326,      0.3817,     0.38015,      0.3786,     0.37705,      0.3755,     0.37395,\n",
              "             0.3724,     0.37084,     0.36929,     0.36774,     0.36619,     0.36464,     0.36309,     0.36153,     0.35998,     0.35843,     0.35688,     0.35533,     0.35378,     0.35222,     0.35067,     0.34912,     0.34757,     0.34602,     0.34447,     0.34291,     0.34136,     0.33981,     0.33826,\n",
              "            0.33671,     0.33516,      0.3336,     0.33205,      0.3305,     0.32895,      0.3274,     0.32585,     0.32429,     0.32274,     0.32119,     0.31964,     0.31809,     0.31654,     0.31498,     0.31343,     0.31188,     0.31033,     0.30878,     0.30723,     0.30567,     0.30412,     0.30257,\n",
              "            0.30102,     0.29947,     0.29792,     0.29636,     0.29481,     0.29326,     0.29171,     0.29016,     0.28861,     0.28705,      0.2855,     0.28395,      0.2824,     0.28085,      0.2793,     0.27774,     0.27619,     0.27464,     0.27309,     0.27154,     0.26999,     0.26843,     0.26688,\n",
              "            0.26533,     0.26378,     0.26223,     0.26068,     0.25912,     0.25757,     0.25602,     0.25447,     0.25292,     0.25137,     0.24982,     0.24826,     0.24671,     0.24516,     0.24361,     0.24206,     0.24051,     0.23895,      0.2374,     0.23585,      0.2343,     0.23275,      0.2312,\n",
              "            0.22964,     0.22809,     0.22654,     0.22499,     0.22344,     0.22189,     0.22033,     0.21878,     0.21723,     0.21568,     0.21413,     0.21258,     0.21102,     0.20947,     0.20792,     0.20637,     0.20482,     0.20327,     0.20171,     0.20016,     0.19861,     0.19706,     0.19551,\n",
              "            0.19396,      0.1924,     0.19085,      0.1893,     0.18775,      0.1862,     0.18465,     0.18309,     0.18154,     0.17999,     0.17844,     0.17689,     0.17534,     0.17378,     0.17223,     0.17068,     0.16913,     0.16758,     0.16603,     0.16447,     0.16292,     0.16137,     0.15982,\n",
              "            0.15827,     0.15672,     0.15516,     0.15361,     0.15206,     0.15051,     0.14896,     0.14741,     0.14585,      0.1443,     0.14275,      0.1412,     0.13965,      0.1381,     0.13654,     0.13499,     0.13344,     0.13189,     0.13034,     0.12879,     0.12723,     0.12568,     0.12413,\n",
              "            0.12258,     0.12103,     0.11948,     0.11793,     0.11637,     0.11482,     0.11327,     0.11172,     0.11017,     0.10862,     0.10706,     0.10551,     0.10396,     0.10241,     0.10086,    0.099305,    0.097754,    0.096202,     0.09465,    0.093099,    0.091547,    0.089995,    0.088444,\n",
              "           0.086892,    0.085341,    0.083789,    0.082237,    0.080686,    0.079134,    0.077582,    0.076031,    0.074479,    0.072927,    0.071376,    0.069824,    0.068272,    0.066721,    0.065169,    0.063617,    0.062066,    0.060514,    0.058963,    0.057411,    0.055859,    0.054308,    0.052756,\n",
              "           0.051204,    0.049653,    0.048101,    0.046549,    0.044998,    0.043446,    0.041894,    0.040343,    0.038791,     0.03724,    0.035688,    0.034136,    0.032585,    0.031033,    0.029481,     0.02793,    0.026378,    0.024826,    0.023275,    0.021723,    0.020171,     0.01862,    0.017068,\n",
              "           0.015516,    0.013965,    0.012413,    0.010862,   0.0093099,   0.0077582,   0.0062066,   0.0046549,   0.0031033,   0.0015516,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[      0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,\n",
              "              0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,\n",
              "              0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,\n",
              "              0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,\n",
              "              0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,\n",
              "              0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,       0.609,     0.60946,     0.60938,     0.60971,      0.6097,     0.61049,\n",
              "            0.61095,     0.61112,     0.61128,     0.61151,     0.61187,     0.61219,     0.61287,     0.61308,     0.61326,     0.61295,     0.61291,     0.61275,     0.61258,     0.61268,     0.61277,     0.61306,     0.61347,     0.61362,       0.614,     0.61457,       0.615,     0.61537,     0.61574,\n",
              "            0.61602,     0.61659,     0.61667,     0.61671,     0.61655,     0.61636,      0.6164,      0.6159,     0.61668,     0.61744,     0.61771,     0.61786,     0.61785,     0.61822,     0.61838,      0.6184,     0.61849,     0.61893,     0.61904,     0.61944,     0.61936,     0.61936,     0.61955,\n",
              "            0.61946,     0.61946,     0.62013,     0.62057,     0.62091,     0.62101,     0.62091,     0.62122,      0.6213,      0.6216,     0.62162,     0.62169,     0.62154,     0.62132,     0.62117,     0.62131,     0.62137,     0.62137,     0.62167,     0.62181,     0.62163,     0.62166,       0.622,\n",
              "            0.62158,     0.62167,      0.6226,     0.62336,     0.62395,     0.62462,     0.62512,      0.6255,     0.62591,     0.62621,     0.62665,     0.62658,     0.62726,     0.62772,     0.62809,     0.62836,     0.62948,     0.63047,     0.63097,     0.63152,     0.63179,     0.63202,     0.63241,\n",
              "            0.63281,      0.6331,     0.63361,     0.63367,     0.63399,     0.63413,     0.63455,     0.63393,     0.63443,     0.63538,     0.63521,     0.63474,     0.63515,     0.63547,     0.63536,     0.63545,     0.63544,     0.63524,     0.63439,     0.63476,     0.63409,     0.63405,     0.63465,\n",
              "            0.63433,     0.63479,     0.63429,     0.63446,     0.63474,     0.63493,     0.63504,     0.63502,     0.63461,     0.63558,       0.636,     0.63622,     0.63683,     0.63672,     0.63637,      0.6363,     0.63596,     0.63641,     0.63669,     0.63707,     0.63664,     0.63674,     0.63693,\n",
              "            0.63548,     0.63547,     0.63568,     0.63524,     0.63475,     0.63315,     0.63224,     0.63077,     0.62933,     0.62835,     0.62623,     0.62589,     0.62501,     0.62404,     0.62254,     0.62165,     0.61996,     0.61963,     0.61838,     0.61652,     0.61664,     0.61378,      0.6134,\n",
              "            0.61236,     0.60961,     0.60821,     0.60673,     0.60457,     0.60292,     0.60091,     0.59973,     0.59718,     0.59314,     0.59177,     0.58785,     0.58602,     0.58339,     0.58176,      0.5787,     0.57747,     0.57451,     0.57131,     0.56861,     0.56608,       0.564,     0.56271,\n",
              "            0.55926,     0.55485,     0.55172,     0.54973,     0.54681,     0.54562,     0.54227,     0.53941,     0.53683,     0.53477,     0.53252,     0.53002,     0.52559,     0.52227,     0.52022,     0.51613,     0.51027,     0.50772,     0.50729,     0.50018,     0.49413,     0.49068,     0.48739,\n",
              "            0.48272,     0.47783,     0.47394,     0.46905,     0.46315,      0.4592,     0.45292,     0.44995,     0.44609,     0.44427,     0.43942,     0.43388,     0.42916,     0.42314,     0.41722,     0.41079,     0.40349,     0.40022,     0.39562,     0.39107,     0.38557,     0.37675,     0.37017,\n",
              "            0.36321,     0.35653,     0.35148,     0.34706,     0.34021,     0.33457,     0.32639,     0.32193,     0.31741,     0.31354,     0.30902,     0.30533,     0.30015,     0.29005,     0.28797,     0.28352,     0.27358,      0.2679,     0.26414,      0.2558,     0.25199,     0.24509,     0.23932,\n",
              "            0.23201,     0.22224,     0.21354,     0.20903,     0.20197,     0.19817,     0.19289,     0.18682,     0.18086,       0.173,     0.16915,     0.16395,     0.15692,     0.15077,     0.14744,     0.14169,     0.13778,       0.131,     0.12314,      0.1162,     0.10989,      0.1079,     0.10087,\n",
              "            0.09593,    0.091626,    0.088719,    0.083162,    0.080391,     0.07508,    0.069714,    0.067726,    0.062932,    0.059364,    0.057662,    0.054269,    0.051986,    0.047988,    0.045789,    0.040977,    0.039784,    0.039492,    0.039201,    0.037394,    0.036286,     0.03508,    0.033349,\n",
              "           0.030929,    0.029495,    0.026126,    0.024995,    0.021312,    0.019584,    0.018115,    0.016082,    0.014721,    0.014376,    0.014119,    0.013861,    0.012704,    0.011951,    0.011563,    0.010571,    0.010276,   0.0094321,   0.0090089,   0.0079876,   0.0076175,   0.0072515,   0.0068987,\n",
              "          0.0065458,   0.0061891,    0.005832,   0.0052835,   0.0046577,   0.0033425,   0.0032135,   0.0031583,    0.003103,   0.0030478,   0.0029925,   0.0029373,   0.0028821,   0.0028268,   0.0027716,   0.0027163,    0.002661,   0.0026058,   0.0025505,   0.0024953,    0.002398,   0.0021495,   0.0019008,\n",
              "          0.0016521,   0.0016251,   0.0016141,   0.0016031,   0.0015921,   0.0015811,   0.0015701,   0.0015591,   0.0015481,   0.0015371,   0.0015261,   0.0015151,   0.0015041,   0.0014931,   0.0014821,   0.0014711,   0.0014601,   0.0014491,   0.0014381,   0.0014271,   0.0014161,   0.0014052,   0.0013942,\n",
              "          0.0013832,   0.0013722,   0.0013612,   0.0013502,   0.0013392,   0.0013282,   0.0013172,   0.0013062,   0.0012952,   0.0012842,   0.0012732,   0.0012622,   0.0012512,   0.0012402,   0.0012292,   0.0012182,   0.0012072,   0.0011962,   0.0011852,   0.0011742,   0.0011632,   0.0011522,   0.0011412,\n",
              "          0.0011302,   0.0011192,   0.0011082,   0.0010972,   0.0010862,   0.0010752,   0.0010642,   0.0010532,   0.0010422,   0.0010312,   0.0010202,   0.0010092,  0.00099817,  0.00098717,  0.00097617,  0.00096517,  0.00095417,  0.00094316,  0.00093216,  0.00092116,  0.00091016,  0.00089915,  0.00088815,\n",
              "         0.00087715,  0.00086614,  0.00085514,  0.00084414,  0.00083313,  0.00082213,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,\n",
              "            0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,\n",
              "            0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,\n",
              "            0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,\n",
              "            0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,\n",
              "            0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61458,     0.61553,     0.61573,     0.61689,     0.61728,     0.61891,\n",
              "            0.61986,     0.62022,     0.62097,      0.6219,     0.62264,      0.6233,     0.62471,     0.62515,     0.62596,      0.6262,     0.62656,     0.62662,     0.62673,     0.62698,     0.62717,     0.62777,     0.62864,     0.62895,     0.62974,     0.63095,     0.63186,     0.63263,     0.63342,\n",
              "            0.63401,     0.63523,     0.63585,     0.63605,     0.63596,     0.63612,      0.6365,     0.63644,     0.63821,     0.63983,     0.64042,     0.64073,     0.64119,     0.64199,     0.64233,     0.64284,     0.64303,     0.64399,     0.64423,     0.64532,     0.64541,     0.64571,     0.64631,\n",
              "            0.64706,     0.64755,     0.64902,        0.65,     0.65073,     0.65096,     0.65124,     0.65192,     0.65258,     0.65374,     0.65428,     0.65445,     0.65461,     0.65471,      0.6548,     0.65509,     0.65541,     0.65574,      0.6564,     0.65672,     0.65683,     0.65739,     0.65826,\n",
              "            0.65804,     0.65895,     0.66104,     0.66329,     0.66462,     0.66667,     0.66782,      0.6692,     0.67015,     0.67137,     0.67237,     0.67329,     0.67541,     0.67682,     0.67845,     0.68074,     0.68336,      0.6857,     0.68689,     0.68819,     0.68942,     0.68994,     0.69146,\n",
              "            0.69241,     0.69406,     0.69608,     0.69681,     0.69758,     0.69793,     0.69894,     0.70041,     0.70165,     0.70397,     0.70503,     0.70543,     0.70646,     0.70743,     0.70758,     0.70828,     0.70943,     0.70976,      0.7101,     0.71144,     0.71127,     0.71182,     0.71332,\n",
              "             0.7138,     0.71559,     0.71627,     0.71737,     0.71808,      0.7193,     0.72014,     0.72117,     0.72168,     0.72486,     0.72664,      0.7279,     0.72949,     0.72979,     0.73034,     0.73083,     0.73131,      0.7325,     0.73395,     0.73496,     0.73662,     0.73902,     0.74097,\n",
              "            0.74209,     0.74344,     0.74594,     0.74736,     0.75079,     0.75218,     0.75428,     0.75594,     0.75755,     0.76051,     0.76228,     0.76523,     0.76761,     0.76839,     0.77017,     0.77223,     0.77358,     0.77543,     0.77709,      0.7793,     0.78234,     0.78252,     0.78522,\n",
              "            0.78811,     0.79097,     0.79239,      0.7946,      0.7966,     0.79883,         0.8,      0.8015,     0.80251,     0.80298,      0.8043,     0.80438,     0.80747,     0.80912,     0.81279,     0.81659,     0.81893,     0.81842,     0.81839,     0.82085,     0.82286,     0.82329,     0.82401,\n",
              "            0.82474,     0.82542,     0.82864,     0.82982,     0.83006,     0.83204,       0.832,      0.8346,     0.83643,     0.83904,     0.83895,     0.84155,     0.84137,      0.8443,      0.8468,     0.84716,     0.84996,     0.85144,     0.85615,     0.85723,     0.85665,     0.85731,     0.86022,\n",
              "            0.86042,     0.85967,     0.86124,     0.86026,     0.86008,     0.86057,     0.86029,     0.86205,     0.86341,     0.86796,     0.86733,      0.8675,     0.86691,     0.86903,     0.86915,      0.8691,     0.86765,     0.86761,     0.86825,     0.87209,     0.87418,     0.87477,     0.87331,\n",
              "            0.87238,     0.86984,     0.86786,     0.86857,     0.86849,     0.86803,     0.86607,     0.86565,     0.86674,     0.86661,     0.86461,     0.86454,     0.86292,       0.859,     0.86481,     0.86618,      0.8668,     0.86775,     0.86973,      0.8675,     0.87166,     0.87232,     0.87153,\n",
              "            0.87659,     0.87365,     0.87363,     0.87355,     0.87462,     0.87781,     0.88318,     0.89192,     0.89129,     0.89316,     0.89424,     0.89455,     0.88997,     0.88566,     0.88713,     0.88688,     0.88384,     0.87816,     0.88508,     0.88311,     0.87749,     0.88084,     0.87901,\n",
              "            0.87808,     0.90074,     0.90476,     0.89878,     0.90335,     0.89696,     0.89861,     0.89928,     0.90874,     0.90605,     0.91224,     0.90711,     0.90332,     0.89591,     0.89135,     0.87986,     0.87663,     0.87582,       0.875,     0.86964,     0.88297,     0.87936,     0.87379,\n",
              "             0.8651,     0.85938,     0.84384,     0.84953,     0.84061,     0.85816,     0.84829,     0.86859,     0.85808,     0.85512,     0.85283,     0.85055,     0.83897,     0.83049,     0.82578,     0.83246,     0.92653,     0.92042,     0.91707,     0.90731,     0.90316,     0.89886,     0.89403,\n",
              "            0.88921,     0.88319,     0.87709,     0.86565,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,\n",
              "            0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,\n",
              "            0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,\n",
              "            0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,\n",
              "            0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,\n",
              "            0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60352,     0.60317,      0.6027,     0.60229,     0.60229,\n",
              "            0.60229,     0.60229,     0.60188,     0.60147,     0.60147,     0.60147,     0.60147,     0.60147,     0.60106,     0.60025,     0.59984,     0.59948,     0.59905,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,     0.59902,\n",
              "            0.59902,     0.59902,     0.59861,     0.59851,     0.59829,     0.59779,     0.59753,     0.59665,     0.59656,     0.59656,     0.59656,     0.59656,     0.59615,     0.59615,     0.59615,     0.59574,     0.59574,     0.59574,     0.59574,     0.59555,     0.59534,     0.59508,     0.59493,\n",
              "            0.59411,      0.5937,      0.5937,      0.5937,      0.5937,      0.5937,     0.59329,     0.59329,     0.59288,     0.59247,     0.59206,     0.59206,     0.59165,     0.59118,     0.59083,     0.59083,     0.59069,     0.59043,     0.59043,     0.59043,     0.59002,     0.58961,     0.58953,\n",
              "            0.58895,     0.58838,     0.58838,     0.58797,     0.58797,     0.58756,     0.58756,     0.58715,     0.58715,     0.58674,     0.58674,     0.58592,     0.58552,     0.58526,      0.5847,     0.58347,     0.58347,     0.58347,     0.58347,     0.58347,     0.58306,     0.58306,     0.58265,\n",
              "            0.58265,     0.58199,     0.58142,     0.58101,     0.58101,     0.58101,     0.58101,     0.57897,     0.57897,     0.57897,     0.57797,     0.57692,     0.57692,      0.5768,     0.57651,      0.5762,     0.57542,     0.57488,     0.57327,       0.573,     0.57201,      0.5716,      0.5716,\n",
              "            0.57079,     0.57038,     0.56914,     0.56874,     0.56874,     0.56827,     0.56792,     0.56725,     0.56628,     0.56588,     0.56547,     0.56506,     0.56506,      0.5647,     0.56383,     0.56342,      0.5626,      0.5626,     0.56219,     0.56219,     0.56056,     0.55933,     0.55851,\n",
              "            0.55565,     0.55488,     0.55382,     0.55237,     0.54978,     0.54664,     0.54419,     0.54116,     0.53823,     0.53532,     0.53138,     0.52948,      0.5271,     0.52534,      0.5224,     0.52022,     0.51724,     0.51596,      0.5135,     0.50999,     0.50886,     0.50491,     0.50327,\n",
              "             0.5007,     0.49591,      0.4935,      0.4907,     0.48714,     0.48417,     0.48117,     0.47912,     0.47552,     0.47025,     0.46809,     0.46318,     0.45989,     0.45614,       0.453,     0.44815,     0.44598,     0.44261,     0.43882,     0.43495,     0.43144,     0.42891,     0.42723,\n",
              "            0.42308,     0.41787,     0.41353,       0.411,     0.40769,     0.40589,     0.40221,     0.39848,     0.39525,     0.39245,     0.39005,     0.38682,     0.38216,     0.37807,     0.37543,     0.37111,     0.36457,      0.3617,     0.36042,     0.35311,      0.3472,      0.3437,     0.34002,\n",
              "            0.33546,     0.33087,     0.32692,     0.32242,      0.3169,     0.31315,     0.30737,     0.30442,     0.30074,     0.29854,     0.29425,     0.28928,     0.28516,     0.27965,     0.27449,     0.26896,     0.26287,      0.2601,     0.25617,     0.25205,     0.24733,     0.24007,     0.23486,\n",
              "            0.22935,     0.22422,     0.22036,     0.21686,     0.21154,     0.20722,     0.20109,     0.19773,     0.19428,     0.19139,     0.18813,     0.18541,     0.18167,     0.17448,     0.17275,      0.1695,     0.16242,      0.1584,     0.15571,     0.15002,     0.14728,     0.14257,     0.13871,\n",
              "            0.13369,     0.12731,     0.12164,     0.11872,     0.11417,     0.11169,     0.10827,     0.10434,     0.10064,    0.095775,    0.093407,    0.090242,    0.086048,    0.082401,    0.080399,    0.076994,    0.074716,    0.070778,    0.066176,    0.062193,    0.058613,     0.05747,    0.053507,\n",
              "           0.050736,    0.048268,    0.046646,    0.043598,    0.042067,     0.03918,    0.036263,    0.035188,    0.032595,    0.030687,    0.029772,    0.027971,    0.026763,    0.024654,    0.023498,    0.020977,    0.020354,    0.020202,     0.02005,    0.019108,    0.018524,    0.017897,    0.016999,\n",
              "           0.015746,    0.015005,    0.013269,    0.012684,    0.010793,   0.0099048,   0.0091554,   0.0081161,   0.0074239,   0.0072492,   0.0071182,   0.0069872,   0.0064004,   0.0060187,   0.0058222,   0.0053191,   0.0051668,   0.0047403,   0.0045267,   0.0040114,   0.0038249,   0.0036404,   0.0034627,\n",
              "           0.003285,   0.0031054,   0.0029257,   0.0026498,   0.0023343,    0.001674,   0.0016093,   0.0015816,   0.0015539,   0.0015262,   0.0014985,   0.0014708,   0.0014431,   0.0014154,   0.0013877,     0.00136,   0.0013323,   0.0013046,   0.0012769,   0.0012492,   0.0012005,   0.0010759,  0.00095131,\n",
              "         0.00082673,   0.0008132,  0.00080769,  0.00080218,  0.00079668,  0.00079117,  0.00078566,  0.00078016,  0.00077465,  0.00076915,  0.00076364,  0.00075813,  0.00075263,  0.00074712,  0.00074161,  0.00073611,   0.0007306,  0.00072509,  0.00071959,  0.00071408,  0.00070858,  0.00070307,  0.00069756,\n",
              "         0.00069206,  0.00068655,  0.00068104,  0.00067554,  0.00067003,  0.00066453,  0.00065902,  0.00065351,  0.00064801,   0.0006425,  0.00063699,  0.00063149,  0.00062598,  0.00062047,  0.00061497,  0.00060946,  0.00060396,  0.00059845,  0.00059294,  0.00058744,  0.00058193,  0.00057642,  0.00057092,\n",
              "         0.00056541,  0.00055991,   0.0005544,  0.00054889,  0.00054339,  0.00053788,  0.00053237,  0.00052687,  0.00052136,  0.00051585,  0.00051035,  0.00050484,  0.00049934,  0.00049383,  0.00048832,  0.00048282,  0.00047731,   0.0004718,   0.0004663,  0.00046079,  0.00045529,  0.00044978,  0.00044427,\n",
              "         0.00043877,  0.00043326,  0.00042775,  0.00042225,  0.00041674,  0.00041123,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.2888649864366184)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.25095])\n",
              "names: {0: 'Insect'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.7054262393104956), 'metrics/recall(B)': np.float64(0.5769230769230769), 'metrics/mAP50(B)': np.float64(0.6301134010646668), 'metrics/mAP50-95(B)': np.float64(0.25094849592239077), 'fitness': np.float64(0.2888649864366184)}\n",
              "save_dir: PosixPath('binary_detector/train302')\n",
              "speed: {'preprocess': 1.249627210199833, 'inference': 642.3848527483642, 'loss': 0.006644055247306824, 'postprocess': 56.875872425735}\n",
              "task: 'detect'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.val(save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "SFbt3D85YFXm",
      "metadata": {
        "id": "SFbt3D85YFXm"
      },
      "outputs": [],
      "source": [
        "# i have the model pre trained\n",
        "model = YOLO(\"/content/binary_detector/train/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e3d61207",
      "metadata": {
        "id": "e3d61207"
      },
      "outputs": [],
      "source": [
        "\n",
        "def crop_predictions(image_path, model, conf_thresh=0.01):\n",
        "    results = model.predict(image_path, conf=conf_thresh,max_det=800)[0]\n",
        "    img = cv2.imread(image_path)\n",
        "    crops = []\n",
        "    for box in results.boxes.xyxy.cpu().numpy():\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        crop = img[y1:y2, x1:x2]\n",
        "        crops.append(crop)\n",
        "    return crops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e7cbe30e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cbe30e",
        "outputId": "7d8e9380-a6ef-4435-db05-d951dd8c59ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "577ee862",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "577ee862",
        "outputId": "bf975cab-0121-444a-cecd-47034965c180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/insects_noclasses-7/test/images/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72.jpg: 1152x1504 18 Insects, 37.8ms\n",
            "Speed: 12.3ms preprocess, 37.8ms inference, 1.9ms postprocess per image at shape (1, 3, 1152, 1504)\n",
            "\n",
            "image 1/1 /content/insects_noclasses-7/test/images/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4.jpg: 1504x1152 602 Insects, 38.2ms\n",
            "Speed: 12.1ms preprocess, 38.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1504, 1152)\n",
            "Saved all cropped images to: /content/our_cropped2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load your trained binary detector\n",
        "binary_detector =model\n",
        "from glob import glob\n",
        "import glob2\n",
        "\n",
        "\"\"\"image_example = glob(\"W5_20220509_2_ST220182_A_JPG.rf.5d17c8a1b9b500761e1d6006c44d8dc9.jpg\")\n",
        "print(image_example)\n",
        "cropped_images = crop_predictions(image_example[0], binary_detector)\n",
        "\"\"\"\n",
        "# Visualize crops\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\"\"\"for i, crop in enumerate(cropped_images[:5]):  # visualize first 5 crops\n",
        "    plt.figure()\n",
        "    plt.imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Cropped Image {i+1}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "image_paths = glob(\"/content/insects_noclasses-7/test/images/*.jpg\")  # Adjust as needed\n",
        "output_dir=\"/content/our_cropped2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Loop through images, crop, and save\n",
        "for image_path in image_paths:\n",
        "    crops = crop_predictions(image_path, binary_detector)\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    for idx, crop in enumerate(crops):\n",
        "        save_path = os.path.join(output_dir, f\"{base_name}_crop{idx}.jpg\")\n",
        "        cv2.imwrite(save_path, crop)\n",
        "\n",
        "print(f\"Saved all cropped images to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6daf8c03",
      "metadata": {
        "id": "6daf8c03"
      },
      "source": [
        "\n",
        "## Next Steps:\n",
        "\n",
        "- Apply augmentations on training cropped images. (per class)\n",
        "- Feed cropped images into our classification model to fine tune it .\n",
        "(Add your augmentation and classification inference code here.)\n",
        "\n",
        "- after fine tuning our classification model we can run a total inference including both models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3dd119f",
      "metadata": {
        "id": "d3dd119f"
      },
      "source": [
        "### Load Roboflow Dataset\n",
        "Replace with your Roboflow API Key and Dataset Information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000c4c2c",
      "metadata": {
        "id": "000c4c2c",
        "outputId": "5ec8b762-653f-4a33-caaf-a2c511f43bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in My-First-Project-12 to yolov8:: 100%|██████████| 10759/10759 [00:01<00:00, 9441.08it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to My-First-Project-12 in yolov8:: 100%|██████████| 100/100 [00:00<00:00, 192.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset location: /hpc/home/ama191/My-First-Project-12\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"TS1niacLXvvWTierCKCT\")\n",
        "project = rf.workspace(\"insectai\").project(\"my-first-project-fmp4x\")\n",
        "dataset = project.version(12).download(\"yolov8\")\n",
        "\n",
        "print(f\"Dataset location: {dataset.location}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f5236b",
      "metadata": {
        "id": "63f5236b"
      },
      "source": [
        "### Imports and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733bff79",
      "metadata": {
        "id": "733bff79"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "label_dir = os.path.join(dataset.location, \"train\", \"labels\")\n",
        "image_dir = os.path.join(dataset.location, \"train\", \"images\")\n",
        "cropped_output_dir = \"cropped_dataset\"\n",
        "os.makedirs(cropped_output_dir, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39f1449",
      "metadata": {
        "id": "c39f1449"
      },
      "source": [
        "### YOLO Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0eb3bd6",
      "metadata": {
        "id": "a0eb3bd6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def yolo_to_xyxy(bbox, img_w, img_h):\n",
        "    x_c, y_c, w, h = bbox\n",
        "    x1 = int((x_c - w / 2) * img_w)\n",
        "    y1 = int((y_c - h / 2) * img_h)\n",
        "    x2 = int((x_c + w / 2) * img_w)\n",
        "    y2 = int((y_c + h / 2) * img_h)\n",
        "    return x1, y1, x2, y2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6449e571",
      "metadata": {
        "id": "6449e571"
      },
      "source": [
        "### Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d0eb30",
      "metadata": {
        "id": "81d0eb30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e49419a",
      "metadata": {
        "id": "2e49419a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def apply_augmentations(crop):\n",
        "    angle = random.choice([0, 90, 180, 270])\n",
        "    angle=angle + (np.random.random_integers(0,15))\n",
        "    if angle:\n",
        "        crop = np.rot90(crop, k=angle // 90)\n",
        "\n",
        "    crop = cv2.GaussianBlur(crop, (5, 5), 0)\n",
        "    noise = np.random.normal(0, 0.1, crop.shape).astype(np.uint8)\n",
        "    crop = cv2.add(crop, noise)\n",
        "\n",
        "    return crop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3e62e1",
      "metadata": {
        "id": "8e3e62e1"
      },
      "source": [
        "# Crop and Augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a46a3dd",
      "metadata": {
        "id": "7a46a3dd",
        "outputId": "3482e4ca-0c5d-4b2e-f74e-b79a037b68b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 1/36 [00:00<00:25,  1.36it/s]This function is deprecated. Please call randint(0, 15 + 1) instead\n",
            "100%|██████████| 36/36 [00:40<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution after cropping and augmentation: Counter({1: 7631, 3: 667, 5: 287, 4: 132, 0: 48, 2: 12})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "rare_classes = [0, 2,4]  # Modify as needed\n",
        "augment_factor = 3\n",
        "\n",
        "class_counter = Counter()\n",
        "\n",
        "for label_file in tqdm(glob(f\"{label_dir}/*.txt\")):\n",
        "    image_file = label_file.replace('/labels/', '/images/').replace('.txt', '.jpg')\n",
        "    img = cv2.imread(image_file)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    with open(label_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            class_id = int(parts[0])\n",
        "            bbox = list(map(float, parts[1:]))\n",
        "            x1, y1, x2, y2 = yolo_to_xyxy(bbox, w, h)\n",
        "\n",
        "            crop = img[y1:y2, x1:x2]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            class_folder = os.path.join(cropped_output_dir, str(class_id))\n",
        "            os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "            filename = os.path.basename(image_file).replace('.jpg', f'_{idx}.jpg')\n",
        "            cv2.imwrite(os.path.join(class_folder, filename), crop)\n",
        "            class_counter[class_id] += 1\n",
        "\n",
        "            if class_id in rare_classes:\n",
        "                for aug_idx in range(augment_factor):\n",
        "                    aug_crop = apply_augmentations(crop)\n",
        "                    aug_filename = os.path.basename(image_file).replace('.jpg', f'_{idx}_aug{aug_idx}.jpg')\n",
        "                    cv2.imwrite(os.path.join(class_folder, aug_filename), aug_crop)\n",
        "                    class_counter[class_id] += 1\n",
        "\n",
        "print(\"Class distribution after cropping and augmentation:\", class_counter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b402eb",
      "metadata": {
        "id": "93b402eb"
      },
      "source": [
        "# Dataset Preparation for YOLO Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae66901",
      "metadata": {
        "id": "5ae66901"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4495443",
      "metadata": {
        "collapsed": true,
        "id": "e4495443",
        "outputId": "4d82e7a9-7bab-4a5c-c086-fa8ec79a4d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_660f25dc76c845e19dd404d51ca3bdee/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_660f25dc76c845e19dd404d51ca3bdee/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-wih17095\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_660f25dc76c845e19dd404d51ca3bdee/\n",
            "  Complete output (15 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/46/1c/395a83ee7b2d2ad7a05b453872053d41449564477c81dc356f720b16eac4/sklearn-0.0.post12.tar.gz#sha256=54cff9e20839b7b202321178228af4d9388bedf78425d9299fd9ee170d68802e (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post11.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_2036601ccb1041a7a25efc6c4356b6b1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_2036601ccb1041a7a25efc6c4356b6b1/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-q8g8vjsx\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_2036601ccb1041a7a25efc6c4356b6b1/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/a4/0b/d1c703256cf293be77b7db44dbef62251fe02a97d0bef981f7120b0b0c0f/sklearn-0.0.post11.tar.gz#sha256=af035c4f0b970b7fc2d3856079aa1aa1032df3d7f65048a9d87114abf13c4629 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_f9ecb379737343029427f195b6fad41b/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_f9ecb379737343029427f195b6fad41b/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-q8_a56za\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_f9ecb379737343029427f195b6fad41b/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b9/0e/b2a4cfaa9e12b9ca4c71507bc26d2c99d75de172c0088c9835a98cf146ff/sklearn-0.0.post10.tar.gz#sha256=d4cd5a2e64b3caaf82cd5e33c46884dfeec5ebf991710d9faeb4fe81cadb3ba6 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_4c608856220542ccab3ba69ea74746f2/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_4c608856220542ccab3ba69ea74746f2/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-rnhi1q7p\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_4c608856220542ccab3ba69ea74746f2/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/28/86/207a003339023247fef1bb5bc9f5093140d17294b2f6d15bfcd4885e469e/sklearn-0.0.post9.tar.gz#sha256=1ff5864cf30489ee48a014fe8f4320d7bb59592392a4ef52ae9d7a37942615ac (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_8c322f37513041b1bfdf567ae2f08dc9/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_8c322f37513041b1bfdf567ae2f08dc9/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-jsks7uge\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_8c322f37513041b1bfdf567ae2f08dc9/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/70/ce/81aa643f3c43488c4a1e417e45f696a61e7ac82b57190fad3c310df2c07b/sklearn-0.0.post7.tar.gz#sha256=1c89020b364fdc3aa2839e0ae34e8f0b406669e4b5c2359dda3ac398f9c76874 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_801c97362fb64732aaa1deb0f6969ab4/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_801c97362fb64732aaa1deb0f6969ab4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-mpnuwx8m\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_801c97362fb64732aaa1deb0f6969ab4/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/7a/93/e0e1b1e98f39dfca7ec9795cb46f6e09e88a2fd5d4a28e4b3d1f618a2aec/sklearn-0.0.post5.tar.gz#sha256=7377c714a03a79bbe9196f435db931fd2a6fa8c68514da7ed3a251fd08c52e2c (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_69cbad628be641ce90f9093a798078ba/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_69cbad628be641ce90f9093a798078ba/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-viw05krk\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_69cbad628be641ce90f9093a798078ba/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/99/b2/165110013aa66fae6fc13918ad0e9de4801e5f1691d371bf8b63328037e6/sklearn-0.0.post4.tar.gz#sha256=0e81ec9c32d4bb418e7be8f1ec1027d174975502dc84cbc4f4564b4cba31e674 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
            "   command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-vovrrbou/sklearn_8383e767c9f14fc5aa06e3337c7551fa/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-vovrrbou/sklearn_8383e767c9f14fc5aa06e3337c7551fa/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-4w0_55ru\n",
            "       cwd: /tmp/pip-install-vovrrbou/sklearn_8383e767c9f14fc5aa06e3337c7551fa/\n",
            "  Complete output (18 lines):\n",
            "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  rather than 'sklearn' for pip commands.\n",
            "  \n",
            "  Here is how to fix this error in the main use cases:\n",
            "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  - if the 'sklearn' package is used by one of your dependencies,\n",
            "    it would be great if you take some time to track which package uses\n",
            "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  - as a last resort, set the environment variable\n",
            "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \n",
            "  More information is available at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \n",
            "  If the previous advice does not cover your use case, feel free to report it at\n",
            "  https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "     |████████████████████████████████| 13.5 MB 2.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.13.1)\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.0.2)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "     |████████████████████████████████| 307 kB 43.3 MB/s            \n",
            "\u001b[?25hUsing legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
            "    Running setup.py install for sklearn ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed joblib-1.5.1 scikit-learn-1.6.1 sklearn-0.0 threadpoolctl-3.6.0\n",
            "Dataset structured for YOLO classification at: classification_dataset\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "classification_dir = \"classification_dataset\"\n",
        "os.makedirs(classification_dir, exist_ok=True)\n",
        "\n",
        "for class_id in os.listdir(cropped_output_dir):\n",
        "    class_path = os.path.join(cropped_output_dir, class_id)\n",
        "    images = glob(os.path.join(class_path, '*.jpg'))\n",
        "\n",
        "    train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    for phase, img_paths in [('train', train_imgs), ('val', val_imgs)]:\n",
        "        phase_class_dir = os.path.join(classification_dir, phase, class_id)\n",
        "        os.makedirs(phase_class_dir, exist_ok=True)\n",
        "\n",
        "        for img_path in img_paths:\n",
        "            shutil.copy(img_path, phase_class_dir)\n",
        "\n",
        "print(\"Dataset structured for YOLO classification at:\", classification_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dba184a",
      "metadata": {
        "id": "6dba184a"
      },
      "source": [
        "# Train YOLO Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bceaec43",
      "metadata": {
        "collapsed": true,
        "id": "bceaec43",
        "outputId": "da1346f2-24a8-4853-a2da-2f51f4fccb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s-cls.pt to 'yolov8s-cls.pt'...\n",
            "100%|██████████████████████████████████████| 12.3M/12.3M [00:00<00:00, 72.7MB/s]\n",
            "Ultralytics 8.3.146 🚀 Python-3.9.19 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=classification_dataset, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /hpc/home/ama191/classification_dataset/train... found 22612 images in 6 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /hpc/home/ama191/classification_dataset/val... found 5657 images in 6 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "Overriding model.yaml nc=1000 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    665606  ultralytics.nn.modules.head.Classify         [512, 6]                      \n",
            "YOLOv8s-cls summary: 56 layers, 5,088,422 parameters, 5,088,422 gradients, 12.6 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.1 ms, read: 0.3±0.0 MB/s, size: 0.8 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /hpc/home/ama191/classification_dataset/train... 22422 images, 1\u001b[0m\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_10.jpg: ignoring corrupt image/label: image size (26, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_106.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_106_aug0.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_106_aug2.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_108.jpg: ignoring corrupt image/label: image size (17, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_108_aug0.jpg: ignoring corrupt image/label: image size (17, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_108_aug1.jpg: ignoring corrupt image/label: image size (8, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_108_aug2.jpg: ignoring corrupt image/label: image size (8, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_10_aug1.jpg: ignoring corrupt image/label: image size (26, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_10_aug2.jpg: ignoring corrupt image/label: image size (7, 26) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_129.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_129_aug0.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_129_aug1.jpg: ignoring corrupt image/label: image size (19, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_129_aug2.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_131.jpg: ignoring corrupt image/label: image size (7, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_131_aug0.jpg: ignoring corrupt image/label: image size (7, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_132.jpg: ignoring corrupt image/label: image size (8, 23) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_132_aug0.jpg: ignoring corrupt image/label: image size (23, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_132_aug1.jpg: ignoring corrupt image/label: image size (23, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_132_aug2.jpg: ignoring corrupt image/label: image size (23, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_167.jpg: ignoring corrupt image/label: image size (23, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_167_aug0.jpg: ignoring corrupt image/label: image size (7, 23) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_167_aug1.jpg: ignoring corrupt image/label: image size (7, 23) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_17.jpg: ignoring corrupt image/label: image size (21, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_17_aug0.jpg: ignoring corrupt image/label: image size (21, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_17_aug1.jpg: ignoring corrupt image/label: image size (21, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_214_aug0.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_214_aug2.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_215_aug0.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_215_aug1.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_233_aug0.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_233_aug2.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_3.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_333.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_333_aug0.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_333_aug1.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_333_aug2.jpg: ignoring corrupt image/label: image size (9, 22) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_341.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_341_aug0.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_341_aug2.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_376.jpg: ignoring corrupt image/label: image size (9, 24) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_376_aug0.jpg: ignoring corrupt image/label: image size (9, 24) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_376_aug1.jpg: ignoring corrupt image/label: image size (9, 24) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_376_aug2.jpg: ignoring corrupt image/label: image size (24, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_455.jpg: ignoring corrupt image/label: image size (21, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_455_aug0.jpg: ignoring corrupt image/label: image size (6, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_455_aug1.jpg: ignoring corrupt image/label: image size (21, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_455_aug2.jpg: ignoring corrupt image/label: image size (6, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_466.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_466_aug0.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_466_aug1.jpg: ignoring corrupt image/label: image size (16, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_466_aug2.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_490_aug0.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_490_aug1.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_490_aug2.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_491.jpg: ignoring corrupt image/label: image size (13, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_491_aug0.jpg: ignoring corrupt image/label: image size (13, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_509.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_509_aug0.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_509_aug1.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_509_aug2.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_522_aug0.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_522_aug1.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_522_aug2.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_523.jpg: ignoring corrupt image/label: image size (11, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_523_aug0.jpg: ignoring corrupt image/label: image size (7, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_523_aug1.jpg: ignoring corrupt image/label: image size (7, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_525.jpg: ignoring corrupt image/label: image size (9, 12) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_525_aug0.jpg: ignoring corrupt image/label: image size (12, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_525_aug2.jpg: ignoring corrupt image/label: image size (12, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_539.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_539_aug1.jpg: ignoring corrupt image/label: image size (9, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_539_aug2.jpg: ignoring corrupt image/label: image size (9, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_64.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_64_aug0.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_64_aug1.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_64_aug2.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_77.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_77_aug0.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_77_aug2.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_208_aug0.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_208_aug1.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_208_aug2.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_420.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_420_aug0.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_420_aug2.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_443.jpg: ignoring corrupt image/label: image size (8, 28) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_443_aug0.jpg: ignoring corrupt image/label: image size (8, 28) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_443_aug1.jpg: ignoring corrupt image/label: image size (28, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_443_aug2.jpg: ignoring corrupt image/label: image size (8, 28) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_60_aug0.jpg: ignoring corrupt image/label: image size (8, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_60_aug1.jpg: ignoring corrupt image/label: image size (18, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_208.jpg: ignoring corrupt image/label: image size (9, 14) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_208_aug0.jpg: ignoring corrupt image/label: image size (9, 14) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_208_aug1.jpg: ignoring corrupt image/label: image size (9, 14) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_420.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_420_aug2.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_60.jpg: ignoring corrupt image/label: image size (19, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_60_aug1.jpg: ignoring corrupt image/label: image size (19, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_60_aug2.jpg: ignoring corrupt image/label: image size (9, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_115.jpg: ignoring corrupt image/label: image size (9, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_115_aug0.jpg: ignoring corrupt image/label: image size (9, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_115_aug1.jpg: ignoring corrupt image/label: image size (9, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_135.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_135_aug0.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_135_aug1.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_135_aug2.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_136.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_136_aug0.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_136_aug1.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_136_aug2.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_266.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_266_aug0.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_266_aug1.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_71.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_71_aug0.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_71_aug1.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_71_aug2.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_159.jpg: ignoring corrupt image/label: image size (15, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_159_aug0.jpg: ignoring corrupt image/label: image size (15, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_159_aug1.jpg: ignoring corrupt image/label: image size (15, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_159_aug2.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_301.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_301_aug2.jpg: ignoring corrupt image/label: image size (9, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_310.jpg: ignoring corrupt image/label: image size (8, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_310_aug0.jpg: ignoring corrupt image/label: image size (11, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_310_aug1.jpg: ignoring corrupt image/label: image size (8, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_310_aug2.jpg: ignoring corrupt image/label: image size (11, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_312_aug0.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_312_aug1.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_312_aug2.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_A_JPG.rf.4e681591685ecff0e64517a3704ef621_96.jpg: ignoring corrupt image/label: image size (9, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_A_JPG.rf.4e681591685ecff0e64517a3704ef621_96_aug0.jpg: ignoring corrupt image/label: image size (9, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_A_JPG.rf.4e681591685ecff0e64517a3704ef621_96_aug2.jpg: ignoring corrupt image/label: image size (9, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_B_JPG.rf.6ebeadc5d93cd3c63e52be0310105c57_15_aug0.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_B_JPG.rf.6ebeadc5d93cd3c63e52be0310105c57_15_aug1.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/0/W5_20220531_2_ST221365_B_JPG.rf.6ebeadc5d93cd3c63e52be0310105c57_15_aug2.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.3cda7ab2090fc27678668a37b97792d3_460.jpg: ignoring corrupt image/label: image size (9, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.3cda7ab2090fc27678668a37b97792d3_6.jpg: ignoring corrupt image/label: image size (18, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_10.jpg: ignoring corrupt image/label: image size (26, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_106.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_108.jpg: ignoring corrupt image/label: image size (17, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_129.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_131.jpg: ignoring corrupt image/label: image size (7, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_132.jpg: ignoring corrupt image/label: image size (8, 23) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_167.jpg: ignoring corrupt image/label: image size (23, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_17.jpg: ignoring corrupt image/label: image size (21, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_214.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_215.jpg: ignoring corrupt image/label: image size (19, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_233.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_3.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_333.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_341.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_376.jpg: ignoring corrupt image/label: image size (9, 24) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_455.jpg: ignoring corrupt image/label: image size (21, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_466.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_522.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_523.jpg: ignoring corrupt image/label: image size (11, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_525.jpg: ignoring corrupt image/label: image size (9, 12) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_539.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_611.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_637.jpg: ignoring corrupt image/label: image size (6, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_64.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_661.jpg: ignoring corrupt image/label: image size (14, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_663.jpg: ignoring corrupt image/label: image size (13, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_701.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_702.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_720.jpg: ignoring corrupt image/label: image size (15, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_77.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_800.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_420.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_443.jpg: ignoring corrupt image/label: image size (8, 28) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_60.jpg: ignoring corrupt image/label: image size (18, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_208.jpg: ignoring corrupt image/label: image size (9, 14) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_420.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_60.jpg: ignoring corrupt image/label: image size (19, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_115.jpg: ignoring corrupt image/label: image size (9, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_135.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_71.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.9e4f65346140095c3258e3f561b982db_135.jpg: ignoring corrupt image/label: image size (7, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.9e4f65346140095c3258e3f561b982db_136.jpg: ignoring corrupt image/label: image size (11, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_A_JPG.rf.9e4f65346140095c3258e3f561b982db_266.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_159.jpg: ignoring corrupt image/label: image size (15, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_312.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220531_2_ST221365_A_JPG.rf.4e681591685ecff0e64517a3704ef621_96.jpg: ignoring corrupt image/label: image size (9, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220531_2_ST221365_B_JPG.rf.6ebeadc5d93cd3c63e52be0310105c57_15.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/1/W5_20220531_2_ST221365_B_JPG.rf.745aec531e99efe2ffed2f469703c47e_15.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/3/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_490.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/3/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_509.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/hpc/home/ama191/classification_dataset/train/3/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_310.jpg: ignoring corrupt image/label: image size (8, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /hpc/home/ama191/classification_dataset/train.cache\n",
            "/hpc/home/ama191/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.7±0.2 ms, read: 0.3±0.1 MB/s, size: 0.8 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /hpc/home/ama191/classification_dataset/val... 5602 images, 55 cor\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_106_aug1.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_10_aug0.jpg: ignoring corrupt image/label: image size (7, 26) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_131_aug1.jpg: ignoring corrupt image/label: image size (7, 21) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_131_aug2.jpg: ignoring corrupt image/label: image size (21, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_167_aug2.jpg: ignoring corrupt image/label: image size (23, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_17_aug2.jpg: ignoring corrupt image/label: image size (21, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_214.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_214_aug1.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_215.jpg: ignoring corrupt image/label: image size (19, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_215_aug2.jpg: ignoring corrupt image/label: image size (8, 19) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_233.jpg: ignoring corrupt image/label: image size (18, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_233_aug1.jpg: ignoring corrupt image/label: image size (9, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_341_aug1.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_3_aug0.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_3_aug1.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_3_aug2.jpg: ignoring corrupt image/label: image size (9, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_490.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_491_aug1.jpg: ignoring corrupt image/label: image size (13, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_491_aug2.jpg: ignoring corrupt image/label: image size (13, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_522.jpg: ignoring corrupt image/label: image size (13, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_523_aug2.jpg: ignoring corrupt image/label: image size (11, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_525_aug1.jpg: ignoring corrupt image/label: image size (9, 12) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_539_aug0.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_77_aug1.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_208.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_420_aug1.jpg: ignoring corrupt image/label: image size (8, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_60.jpg: ignoring corrupt image/label: image size (18, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_60_aug2.jpg: ignoring corrupt image/label: image size (8, 18) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_208_aug2.jpg: ignoring corrupt image/label: image size (14, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_420_aug0.jpg: ignoring corrupt image/label: image size (9, 17) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_420_aug1.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220509_5_ST220185_B_JPG.rf.95b93ec50e09f45a53fe62559c0b50eb_60_aug0.jpg: ignoring corrupt image/label: image size (19, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_115_aug2.jpg: ignoring corrupt image/label: image size (13, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_266_aug2.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_301_aug0.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_301_aug1.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_312.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220531_2_ST221365_A_JPG.rf.4e681591685ecff0e64517a3704ef621_96_aug1.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/0/W5_20220531_2_ST221365_B_JPG.rf.6ebeadc5d93cd3c63e52be0310105c57_15.jpg: ignoring corrupt image/label: image size (8, 20) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.3cda7ab2090fc27678668a37b97792d3_1.jpg: ignoring corrupt image/label: image size (25, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.3cda7ab2090fc27678668a37b97792d3_17.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.3cda7ab2090fc27678668a37b97792d3_606.jpg: ignoring corrupt image/label: image size (14, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_491.jpg: ignoring corrupt image/label: image size (13, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_581.jpg: ignoring corrupt image/label: image size (16, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_641.jpg: ignoring corrupt image/label: image size (8, 15) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_795.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_796.jpg: ignoring corrupt image/label: image size (8, 12) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.7f11db891c5f43d3e107f6d7b6c36d20_816.jpg: ignoring corrupt image/label: image size (16, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.bd8466d7327e1d7ebcf6b7e0ed80c590_491.jpg: ignoring corrupt image/label: image size (14, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_3_ST220183_B_JPG.rf.bd8466d7327e1d7ebcf6b7e0ed80c590_637.jpg: ignoring corrupt image/label: image size (9, 14) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220509_5_ST220185_B_JPG.rf.14b8d86a03aa64f2bf8adbe5fd4d328b_208.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_136.jpg: ignoring corrupt image/label: image size (9, 11) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220523_5_ST220265_A_JPG.rf.4041288695e5e90b82c068774bc0d0a2_266.jpg: ignoring corrupt image/label: image size (20, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/1/W5_20220523_5_ST220265_A_JPG.rf.9e4f65346140095c3258e3f561b982db_115.jpg: ignoring corrupt image/label: image size (13, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/hpc/home/ama191/classification_dataset/val/3/W5_20220523_5_ST220265_B_JPG.rf.6acd2c5f3245bc6074a53b88bc63ccd5_301.jpg: ignoring corrupt image/label: image size (15, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /hpc/home/ama191/classification_dataset/val.cache\n",
            "/hpc/home/ama191/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "/hpc/home/ama191/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       1/10     0.564G     0.6926          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:22<00\n",
            "                   all      0.785      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       2/10     0.578G     0.5352          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all      0.784          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       3/10      0.59G     0.5161          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all      0.789          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       4/10     0.604G     0.4953          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all      0.785          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       5/10     0.613G     0.4692          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:21<00\n",
            "                   all      0.779          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       6/10     0.703G     0.4588          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all      0.786          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       7/10     0.715G      0.445          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:18<00\n",
            "                   all      0.788          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       8/10     0.727G     0.4339          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all       0.79          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       9/10     0.738G     0.4177          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:19<00\n",
            "                   all      0.788          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      10/10      0.75G     0.4075          6        224: 100%|██████████| 1402/1\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:20<00\n",
            "                   all      0.788          1\n",
            "\n",
            "10 epochs completed in 0.327 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 10.3MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 10.3MB\n",
            "\n",
            "Validating runs/classify/train/weights/best.pt...\n",
            "Ultralytics 8.3.146 🚀 Python-3.9.19 torch-2.7.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
            "YOLOv8s-cls summary (fused): 30 layers, 5,082,886 parameters, 0 gradients, 12.5 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /hpc/home/ama191/classification_dataset/train... found 22612 images in 6 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /hpc/home/ama191/classification_dataset/val... found 5657 images in 6 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 176/176 [00:18<00\n",
            "                   all       0.79          1\n",
            "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!yolo task=classify mode=train model=yolov8s-cls.pt data={classification_dir} epochs=10 imgsz=224\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "43ff0a59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "43ff0a59",
        "outputId": "0db3a418-1a0c-4d2f-c359-34f7642562d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.156 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "YOLOv8s-cls summary (fused): 30 layers, 5,082,886 parameters, 0 gradients, 12.5 GFLOPs\n",
            "\n",
            "image 1/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop0.jpg: 224x224 0 0.38, 1 0.34, 3 0.22, 4 0.04, 5 0.02, 4.4ms\n",
            "image 2/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop1.jpg: 224x224 1 0.63, 0 0.35, 3 0.02, 5 0.00, 2 0.00, 3.6ms\n",
            "image 3/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop10.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.01, 2 0.00, 3.3ms\n",
            "image 4/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop100.jpg: 224x224 1 0.52, 0 0.35, 3 0.13, 5 0.00, 2 0.00, 3.2ms\n",
            "image 5/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop101.jpg: 224x224 0 0.36, 1 0.36, 3 0.28, 5 0.00, 4 0.00, 3.2ms\n",
            "image 6/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop102.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 7/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop103.jpg: 224x224 1 0.54, 0 0.42, 3 0.04, 5 0.00, 2 0.00, 3.5ms\n",
            "image 8/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop104.jpg: 224x224 1 0.54, 0 0.41, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 9/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop105.jpg: 224x224 0 0.58, 1 0.40, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 10/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop106.jpg: 224x224 1 0.50, 0 0.48, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 11/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop107.jpg: 224x224 1 0.42, 0 0.36, 3 0.21, 5 0.01, 4 0.00, 3.1ms\n",
            "image 12/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop108.jpg: 224x224 0 0.39, 1 0.30, 3 0.24, 5 0.08, 4 0.00, 3.1ms\n",
            "image 13/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop109.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 14/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop11.jpg: 224x224 1 0.61, 0 0.38, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 15/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop110.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 16/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop111.jpg: 224x224 1 0.56, 0 0.39, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 17/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop112.jpg: 224x224 1 0.65, 0 0.32, 3 0.03, 5 0.00, 2 0.00, 3.0ms\n",
            "image 18/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop113.jpg: 224x224 1 0.56, 0 0.38, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 19/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop114.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 20/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop115.jpg: 224x224 0 0.34, 3 0.32, 1 0.22, 5 0.10, 4 0.03, 3.1ms\n",
            "image 21/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop116.jpg: 224x224 1 0.52, 0 0.36, 3 0.12, 5 0.00, 2 0.00, 3.1ms\n",
            "image 22/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop117.jpg: 224x224 1 0.58, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 23/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop118.jpg: 224x224 1 0.52, 0 0.42, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 24/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop119.jpg: 224x224 1 0.61, 0 0.37, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 25/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop12.jpg: 224x224 1 0.54, 0 0.42, 3 0.03, 5 0.01, 2 0.00, 3.1ms\n",
            "image 26/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop120.jpg: 224x224 1 0.55, 0 0.40, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 27/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop121.jpg: 224x224 1 0.48, 0 0.39, 3 0.08, 5 0.05, 4 0.00, 3.1ms\n",
            "image 28/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop122.jpg: 224x224 1 0.65, 0 0.34, 3 0.01, 5 0.00, 2 0.00, 3.8ms\n",
            "image 29/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop123.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 30/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop124.jpg: 224x224 1 0.61, 0 0.37, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 31/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop125.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 32/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop126.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 33/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop127.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 34/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop128.jpg: 224x224 0 0.40, 1 0.28, 5 0.19, 3 0.13, 4 0.00, 3.1ms\n",
            "image 35/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop129.jpg: 224x224 1 0.56, 0 0.38, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 36/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop13.jpg: 224x224 1 0.55, 0 0.38, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 37/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop130.jpg: 224x224 1 0.57, 0 0.38, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 38/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop131.jpg: 224x224 1 0.57, 0 0.39, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 39/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop132.jpg: 224x224 1 0.62, 0 0.33, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 40/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop133.jpg: 224x224 1 0.54, 0 0.39, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 41/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop134.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.00, 4 0.00, 3.4ms\n",
            "image 42/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop135.jpg: 224x224 1 0.56, 0 0.40, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 43/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop136.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 44/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop137.jpg: 224x224 1 0.56, 0 0.37, 3 0.06, 5 0.01, 4 0.00, 3.1ms\n",
            "image 45/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop138.jpg: 224x224 1 0.65, 0 0.33, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 46/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop139.jpg: 224x224 1 0.58, 0 0.39, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 47/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop14.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 48/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop140.jpg: 224x224 1 0.61, 0 0.37, 3 0.01, 5 0.00, 4 0.00, 3.1ms\n",
            "image 49/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop141.jpg: 224x224 1 0.51, 0 0.40, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 50/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop142.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.9ms\n",
            "image 51/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop143.jpg: 224x224 1 0.48, 0 0.38, 3 0.13, 5 0.00, 4 0.00, 3.1ms\n",
            "image 52/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop144.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 53/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop145.jpg: 224x224 1 0.60, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 54/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop146.jpg: 224x224 1 0.64, 0 0.35, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 55/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop147.jpg: 224x224 1 0.46, 0 0.44, 3 0.07, 5 0.02, 4 0.00, 3.1ms\n",
            "image 56/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop148.jpg: 224x224 1 0.50, 0 0.44, 3 0.06, 5 0.00, 4 0.00, 3.1ms\n",
            "image 57/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop149.jpg: 224x224 1 0.54, 0 0.42, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 58/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop15.jpg: 224x224 1 0.55, 0 0.40, 3 0.03, 5 0.01, 2 0.00, 3.1ms\n",
            "image 59/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop150.jpg: 224x224 1 0.47, 0 0.37, 3 0.16, 5 0.00, 2 0.00, 3.1ms\n",
            "image 60/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop151.jpg: 224x224 1 0.61, 0 0.35, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 61/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop152.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 62/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop153.jpg: 224x224 1 0.46, 0 0.36, 3 0.17, 5 0.01, 2 0.00, 3.1ms\n",
            "image 63/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop154.jpg: 224x224 1 0.55, 0 0.37, 3 0.07, 5 0.01, 2 0.00, 3.1ms\n",
            "image 64/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop155.jpg: 224x224 1 0.52, 0 0.41, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 65/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop156.jpg: 224x224 0 0.53, 1 0.41, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 66/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop157.jpg: 224x224 1 0.40, 0 0.38, 3 0.18, 5 0.04, 4 0.00, 3.1ms\n",
            "image 67/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop158.jpg: 224x224 1 0.50, 0 0.43, 3 0.07, 5 0.01, 2 0.00, 3.4ms\n",
            "image 68/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop159.jpg: 224x224 1 0.58, 0 0.34, 3 0.08, 5 0.00, 2 0.00, 3.5ms\n",
            "image 69/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop16.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 2 0.00, 3.6ms\n",
            "image 70/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop160.jpg: 224x224 1 0.49, 0 0.38, 3 0.10, 5 0.02, 2 0.00, 3.1ms\n",
            "image 71/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop161.jpg: 224x224 0 0.43, 1 0.28, 5 0.15, 3 0.14, 4 0.00, 3.1ms\n",
            "image 72/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop162.jpg: 224x224 1 0.54, 0 0.36, 3 0.10, 5 0.00, 4 0.00, 3.8ms\n",
            "image 73/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop163.jpg: 224x224 1 0.60, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 74/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop164.jpg: 224x224 1 0.59, 0 0.36, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 75/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop165.jpg: 224x224 1 0.60, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 76/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop166.jpg: 224x224 1 0.58, 0 0.39, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 77/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop167.jpg: 224x224 0 0.46, 3 0.27, 1 0.19, 5 0.08, 4 0.00, 3.1ms\n",
            "image 78/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop168.jpg: 224x224 1 0.58, 0 0.37, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 79/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop169.jpg: 224x224 1 0.54, 0 0.39, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 80/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop17.jpg: 224x224 1 0.52, 0 0.43, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 81/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop170.jpg: 224x224 1 0.58, 0 0.36, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 82/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop171.jpg: 224x224 1 0.55, 0 0.39, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 83/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop172.jpg: 224x224 1 0.58, 0 0.35, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 84/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop173.jpg: 224x224 1 0.49, 0 0.36, 3 0.15, 5 0.00, 2 0.00, 3.1ms\n",
            "image 85/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop174.jpg: 224x224 1 0.55, 0 0.42, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 86/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop175.jpg: 224x224 1 0.37, 3 0.33, 0 0.30, 5 0.01, 4 0.00, 3.1ms\n",
            "image 87/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop176.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 88/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop177.jpg: 224x224 1 0.59, 0 0.36, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 89/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop178.jpg: 224x224 1 0.55, 0 0.40, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 90/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop179.jpg: 224x224 1 0.58, 0 0.36, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 91/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop18.jpg: 224x224 1 0.61, 0 0.37, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 92/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop180.jpg: 224x224 1 0.52, 0 0.39, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 93/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop181.jpg: 224x224 1 0.59, 0 0.38, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 94/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop182.jpg: 224x224 1 0.55, 0 0.36, 3 0.08, 5 0.01, 2 0.00, 4.6ms\n",
            "image 95/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop183.jpg: 224x224 1 0.59, 0 0.36, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 96/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop184.jpg: 224x224 0 0.39, 1 0.24, 5 0.23, 3 0.14, 4 0.00, 3.1ms\n",
            "image 97/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop185.jpg: 224x224 1 0.54, 0 0.41, 3 0.03, 5 0.02, 4 0.00, 3.1ms\n",
            "image 98/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop186.jpg: 224x224 1 0.53, 0 0.37, 3 0.10, 5 0.01, 4 0.00, 3.1ms\n",
            "image 99/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop187.jpg: 224x224 1 0.47, 0 0.41, 3 0.12, 5 0.00, 2 0.00, 3.1ms\n",
            "image 100/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop188.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.02, 4 0.00, 3.1ms\n",
            "image 101/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop189.jpg: 224x224 1 0.47, 0 0.37, 3 0.16, 5 0.00, 4 0.00, 3.1ms\n",
            "image 102/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop19.jpg: 224x224 1 0.68, 0 0.32, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 103/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop190.jpg: 224x224 1 0.59, 0 0.38, 3 0.03, 5 0.00, 4 0.00, 3.2ms\n",
            "image 104/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop191.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 105/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop192.jpg: 224x224 0 0.49, 1 0.48, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 106/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop193.jpg: 224x224 1 0.53, 0 0.39, 3 0.06, 5 0.02, 2 0.00, 3.1ms\n",
            "image 107/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop194.jpg: 224x224 1 0.53, 0 0.38, 3 0.07, 5 0.02, 2 0.00, 3.1ms\n",
            "image 108/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop195.jpg: 224x224 1 0.55, 0 0.41, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 109/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop196.jpg: 224x224 1 0.60, 0 0.36, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 110/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop197.jpg: 224x224 1 0.61, 0 0.36, 3 0.02, 5 0.01, 4 0.00, 3.4ms\n",
            "image 111/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop198.jpg: 224x224 1 0.53, 0 0.40, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 112/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop199.jpg: 224x224 1 0.56, 0 0.40, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 113/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop2.jpg: 224x224 1 0.60, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 114/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop20.jpg: 224x224 1 0.54, 0 0.39, 3 0.05, 5 0.02, 2 0.00, 3.2ms\n",
            "image 115/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop200.jpg: 224x224 1 0.54, 0 0.38, 3 0.05, 5 0.03, 4 0.00, 3.1ms\n",
            "image 116/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop201.jpg: 224x224 1 0.58, 0 0.35, 3 0.05, 5 0.02, 4 0.00, 3.9ms\n",
            "image 117/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop202.jpg: 224x224 1 0.53, 0 0.36, 3 0.09, 5 0.01, 4 0.00, 3.4ms\n",
            "image 118/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop203.jpg: 224x224 1 0.53, 0 0.43, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 119/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop204.jpg: 224x224 1 0.53, 0 0.41, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 120/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop205.jpg: 224x224 1 0.60, 0 0.37, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 121/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop206.jpg: 224x224 1 0.55, 0 0.41, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 122/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop207.jpg: 224x224 1 0.52, 0 0.43, 3 0.03, 5 0.01, 2 0.00, 3.1ms\n",
            "image 123/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop208.jpg: 224x224 1 0.45, 0 0.43, 3 0.10, 5 0.01, 2 0.00, 3.1ms\n",
            "image 124/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop209.jpg: 224x224 1 0.57, 0 0.34, 3 0.05, 5 0.04, 2 0.00, 3.1ms\n",
            "image 125/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop21.jpg: 224x224 1 0.57, 0 0.38, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 126/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop210.jpg: 224x224 1 0.55, 0 0.32, 3 0.13, 5 0.00, 2 0.00, 3.1ms\n",
            "image 127/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop211.jpg: 224x224 0 0.41, 1 0.34, 3 0.22, 5 0.03, 4 0.00, 3.1ms\n",
            "image 128/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop212.jpg: 224x224 0 0.42, 1 0.39, 3 0.19, 5 0.00, 2 0.00, 3.1ms\n",
            "image 129/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop213.jpg: 224x224 1 0.58, 0 0.36, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 130/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop214.jpg: 224x224 1 0.59, 0 0.36, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 131/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop215.jpg: 224x224 1 0.56, 0 0.39, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 132/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop216.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 133/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop217.jpg: 224x224 1 0.65, 0 0.33, 3 0.02, 5 0.00, 4 0.00, 3.2ms\n",
            "image 134/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop218.jpg: 224x224 1 0.57, 0 0.37, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 135/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop219.jpg: 224x224 0 0.47, 1 0.43, 3 0.08, 5 0.03, 2 0.00, 3.1ms\n",
            "image 136/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop22.jpg: 224x224 1 0.57, 0 0.39, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 137/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop220.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 138/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop221.jpg: 224x224 1 0.51, 0 0.40, 3 0.06, 5 0.04, 2 0.00, 3.1ms\n",
            "image 139/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop222.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 4 0.00, 4.4ms\n",
            "image 140/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop223.jpg: 224x224 1 0.54, 0 0.41, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 141/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop224.jpg: 224x224 0 0.49, 1 0.45, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 142/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop225.jpg: 224x224 0 0.46, 1 0.45, 3 0.07, 5 0.02, 2 0.00, 3.1ms\n",
            "image 143/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop226.jpg: 224x224 1 0.61, 0 0.34, 3 0.05, 5 0.00, 2 0.00, 3.2ms\n",
            "image 144/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop227.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 145/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop228.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 146/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop229.jpg: 224x224 0 0.42, 1 0.30, 3 0.23, 5 0.04, 4 0.01, 3.1ms\n",
            "image 147/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop23.jpg: 224x224 1 0.54, 0 0.39, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 148/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop230.jpg: 224x224 1 0.50, 0 0.42, 3 0.07, 5 0.01, 2 0.00, 3.1ms\n",
            "image 149/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop231.jpg: 224x224 1 0.53, 0 0.39, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 150/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop232.jpg: 224x224 3 0.35, 1 0.34, 0 0.31, 5 0.00, 2 0.00, 3.1ms\n",
            "image 151/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop233.jpg: 224x224 1 0.62, 0 0.35, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 152/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop234.jpg: 224x224 1 0.64, 0 0.35, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 153/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop235.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 154/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop236.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 155/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop237.jpg: 224x224 1 0.57, 0 0.39, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 156/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop238.jpg: 224x224 1 0.50, 0 0.37, 3 0.13, 5 0.00, 2 0.00, 3.1ms\n",
            "image 157/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop239.jpg: 224x224 1 0.59, 0 0.36, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 158/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop24.jpg: 224x224 1 0.58, 0 0.36, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 159/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop240.jpg: 224x224 0 0.37, 1 0.36, 3 0.15, 5 0.12, 4 0.00, 3.1ms\n",
            "image 160/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop241.jpg: 224x224 1 0.62, 0 0.36, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 161/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop242.jpg: 224x224 1 0.57, 0 0.36, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 162/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop243.jpg: 224x224 1 0.61, 0 0.35, 3 0.03, 5 0.00, 4 0.00, 3.8ms\n",
            "image 163/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop244.jpg: 224x224 1 0.57, 0 0.36, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 164/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop245.jpg: 224x224 1 0.51, 0 0.33, 3 0.15, 5 0.00, 2 0.00, 3.1ms\n",
            "image 165/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop246.jpg: 224x224 1 0.53, 0 0.39, 3 0.06, 5 0.02, 4 0.00, 3.1ms\n",
            "image 166/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop247.jpg: 224x224 1 0.56, 0 0.42, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 167/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop248.jpg: 224x224 1 0.56, 0 0.39, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 168/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop249.jpg: 224x224 1 0.58, 0 0.37, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 169/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop25.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 170/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop250.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 171/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop251.jpg: 224x224 1 0.56, 0 0.38, 3 0.04, 5 0.02, 4 0.00, 3.1ms\n",
            "image 172/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop252.jpg: 224x224 1 0.59, 0 0.39, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 173/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop253.jpg: 224x224 1 0.59, 0 0.39, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 174/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop254.jpg: 224x224 1 0.55, 0 0.40, 3 0.02, 5 0.02, 4 0.00, 3.1ms\n",
            "image 175/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop255.jpg: 224x224 1 0.57, 0 0.39, 3 0.03, 5 0.01, 2 0.00, 3.1ms\n",
            "image 176/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop256.jpg: 224x224 0 0.43, 1 0.40, 3 0.12, 5 0.06, 4 0.00, 3.1ms\n",
            "image 177/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop257.jpg: 224x224 0 0.46, 5 0.34, 3 0.14, 1 0.05, 4 0.00, 3.1ms\n",
            "image 178/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop258.jpg: 224x224 1 0.60, 0 0.37, 3 0.02, 5 0.00, 2 0.00, 3.2ms\n",
            "image 179/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop259.jpg: 224x224 1 0.56, 0 0.37, 3 0.07, 5 0.00, 2 0.00, 3.7ms\n",
            "image 180/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop26.jpg: 224x224 1 0.66, 0 0.32, 3 0.02, 5 0.00, 4 0.00, 3.7ms\n",
            "image 181/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop260.jpg: 224x224 1 0.54, 0 0.44, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 182/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop261.jpg: 224x224 1 0.62, 0 0.35, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 183/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop262.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 184/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop263.jpg: 224x224 1 0.61, 0 0.35, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 185/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop264.jpg: 224x224 1 0.53, 0 0.39, 3 0.06, 5 0.01, 2 0.00, 3.8ms\n",
            "image 186/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop265.jpg: 224x224 1 0.48, 0 0.46, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 187/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop266.jpg: 224x224 1 0.64, 0 0.33, 3 0.02, 5 0.00, 2 0.00, 3.2ms\n",
            "image 188/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop267.jpg: 224x224 0 0.39, 1 0.35, 5 0.15, 3 0.11, 4 0.00, 3.1ms\n",
            "image 189/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop268.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 2 0.00, 3.2ms\n",
            "image 190/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop269.jpg: 224x224 1 0.54, 0 0.41, 3 0.05, 5 0.01, 4 0.00, 3.2ms\n",
            "image 191/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop27.jpg: 224x224 1 0.53, 0 0.40, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 192/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop270.jpg: 224x224 1 0.64, 0 0.35, 3 0.02, 5 0.00, 2 0.00, 3.2ms\n",
            "image 193/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop271.jpg: 224x224 1 0.55, 0 0.39, 3 0.03, 5 0.02, 4 0.00, 3.2ms\n",
            "image 194/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop272.jpg: 224x224 1 0.53, 0 0.41, 3 0.06, 5 0.01, 4 0.00, 3.1ms\n",
            "image 195/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop273.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 196/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop274.jpg: 224x224 1 0.56, 0 0.37, 3 0.06, 5 0.01, 2 0.00, 3.2ms\n",
            "image 197/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop275.jpg: 224x224 1 0.58, 0 0.37, 3 0.05, 5 0.00, 4 0.00, 3.2ms\n",
            "image 198/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop276.jpg: 224x224 1 0.53, 0 0.42, 3 0.05, 5 0.00, 2 0.00, 3.2ms\n",
            "image 199/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop277.jpg: 224x224 1 0.49, 0 0.44, 3 0.06, 5 0.01, 2 0.00, 3.6ms\n",
            "image 200/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop278.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 201/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop279.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 202/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop28.jpg: 224x224 1 0.56, 0 0.41, 3 0.03, 5 0.00, 4 0.00, 3.2ms\n",
            "image 203/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop280.jpg: 224x224 1 0.54, 0 0.37, 3 0.08, 5 0.00, 2 0.00, 3.2ms\n",
            "image 204/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop281.jpg: 224x224 0 0.51, 3 0.22, 5 0.20, 1 0.07, 4 0.00, 3.1ms\n",
            "image 205/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop282.jpg: 224x224 3 0.42, 1 0.29, 0 0.28, 5 0.01, 4 0.00, 3.2ms\n",
            "image 206/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop283.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 207/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop284.jpg: 224x224 1 0.57, 0 0.36, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 208/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop285.jpg: 224x224 1 0.52, 0 0.43, 3 0.04, 5 0.01, 2 0.00, 3.9ms\n",
            "image 209/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop286.jpg: 224x224 0 0.35, 1 0.24, 5 0.23, 3 0.18, 4 0.00, 3.2ms\n",
            "image 210/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop287.jpg: 224x224 5 0.43, 0 0.25, 3 0.22, 1 0.09, 4 0.00, 3.1ms\n",
            "image 211/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop288.jpg: 224x224 1 0.53, 0 0.39, 3 0.07, 5 0.02, 2 0.00, 3.2ms\n",
            "image 212/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop289.jpg: 224x224 1 0.46, 0 0.36, 3 0.18, 5 0.00, 2 0.00, 3.2ms\n",
            "image 213/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop29.jpg: 224x224 0 0.45, 1 0.33, 3 0.13, 5 0.08, 4 0.00, 3.2ms\n",
            "image 214/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop290.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.01, 4 0.00, 3.2ms\n",
            "image 215/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop291.jpg: 224x224 1 0.54, 0 0.38, 3 0.07, 5 0.01, 2 0.00, 3.2ms\n",
            "image 216/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop292.jpg: 224x224 0 0.47, 1 0.34, 5 0.10, 3 0.08, 4 0.00, 3.4ms\n",
            "image 217/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop293.jpg: 224x224 1 0.59, 0 0.36, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 218/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop294.jpg: 224x224 1 0.61, 0 0.34, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 219/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop295.jpg: 224x224 1 0.52, 0 0.43, 3 0.04, 5 0.01, 4 0.00, 3.4ms\n",
            "image 220/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop296.jpg: 224x224 1 0.55, 0 0.40, 3 0.03, 5 0.02, 4 0.00, 3.2ms\n",
            "image 221/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop297.jpg: 224x224 1 0.56, 0 0.41, 3 0.02, 5 0.01, 4 0.00, 3.2ms\n",
            "image 222/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop298.jpg: 224x224 1 0.57, 0 0.40, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 223/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop299.jpg: 224x224 1 0.57, 0 0.41, 3 0.02, 5 0.01, 4 0.00, 3.2ms\n",
            "image 224/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop3.jpg: 224x224 1 0.39, 0 0.35, 3 0.23, 5 0.03, 2 0.00, 3.2ms\n",
            "image 225/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop30.jpg: 224x224 1 0.51, 0 0.38, 3 0.10, 5 0.00, 4 0.00, 3.2ms\n",
            "image 226/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop300.jpg: 224x224 5 0.57, 0 0.23, 3 0.14, 1 0.06, 4 0.00, 3.1ms\n",
            "image 227/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop301.jpg: 224x224 0 0.36, 5 0.26, 3 0.20, 1 0.18, 4 0.00, 3.2ms\n",
            "image 228/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop302.jpg: 224x224 1 0.52, 0 0.40, 3 0.06, 5 0.01, 4 0.00, 3.1ms\n",
            "image 229/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop303.jpg: 224x224 1 0.54, 0 0.43, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 230/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop304.jpg: 224x224 1 0.55, 0 0.36, 3 0.09, 5 0.00, 2 0.00, 3.2ms\n",
            "image 231/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop305.jpg: 224x224 0 0.42, 1 0.39, 3 0.19, 5 0.00, 4 0.00, 3.9ms\n",
            "image 232/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop306.jpg: 224x224 1 0.59, 0 0.37, 3 0.02, 5 0.02, 4 0.00, 3.2ms\n",
            "image 233/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop307.jpg: 224x224 1 0.53, 0 0.38, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 234/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop308.jpg: 224x224 0 0.47, 1 0.31, 3 0.18, 5 0.03, 4 0.02, 3.2ms\n",
            "image 235/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop309.jpg: 224x224 1 0.50, 0 0.43, 3 0.05, 5 0.01, 4 0.00, 3.2ms\n",
            "image 236/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop31.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 2 0.00, 3.2ms\n",
            "image 237/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop310.jpg: 224x224 0 0.48, 1 0.47, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 238/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop311.jpg: 224x224 1 0.53, 0 0.41, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 239/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop312.jpg: 224x224 1 0.53, 0 0.37, 3 0.10, 5 0.00, 2 0.00, 3.1ms\n",
            "image 240/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop313.jpg: 224x224 1 0.56, 0 0.39, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 241/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop314.jpg: 224x224 1 0.59, 0 0.34, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 242/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop315.jpg: 224x224 0 0.41, 1 0.31, 3 0.15, 5 0.13, 4 0.00, 3.2ms\n",
            "image 243/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop316.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.00, 2 0.00, 3.2ms\n",
            "image 244/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop317.jpg: 224x224 0 0.35, 1 0.27, 3 0.27, 5 0.07, 4 0.05, 3.1ms\n",
            "image 245/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop318.jpg: 224x224 0 0.39, 3 0.23, 5 0.19, 1 0.18, 4 0.00, 3.1ms\n",
            "image 246/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop319.jpg: 224x224 1 0.47, 0 0.38, 3 0.14, 5 0.01, 2 0.00, 3.1ms\n",
            "image 247/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop32.jpg: 224x224 1 0.62, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.2ms\n",
            "image 248/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop320.jpg: 224x224 1 0.58, 0 0.37, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 249/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop321.jpg: 224x224 1 0.58, 0 0.40, 3 0.01, 5 0.01, 4 0.00, 3.1ms\n",
            "image 250/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop322.jpg: 224x224 1 0.53, 0 0.39, 3 0.08, 5 0.00, 4 0.00, 3.2ms\n",
            "image 251/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop323.jpg: 224x224 1 0.47, 0 0.41, 3 0.11, 5 0.01, 2 0.00, 3.2ms\n",
            "image 252/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop324.jpg: 224x224 1 0.50, 0 0.44, 3 0.03, 5 0.03, 4 0.00, 3.2ms\n",
            "image 253/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop325.jpg: 224x224 1 0.60, 0 0.36, 3 0.03, 5 0.01, 4 0.00, 3.2ms\n",
            "image 254/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop326.jpg: 224x224 1 0.57, 0 0.36, 3 0.07, 5 0.00, 4 0.00, 3.8ms\n",
            "image 255/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop327.jpg: 224x224 1 0.53, 0 0.40, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 256/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop328.jpg: 224x224 3 0.36, 0 0.32, 1 0.31, 5 0.01, 4 0.00, 3.2ms\n",
            "image 257/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop329.jpg: 224x224 1 0.50, 0 0.43, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 258/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop33.jpg: 224x224 1 0.49, 0 0.36, 3 0.15, 5 0.00, 2 0.00, 3.1ms\n",
            "image 259/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop330.jpg: 224x224 1 0.48, 0 0.40, 3 0.11, 5 0.01, 2 0.00, 3.1ms\n",
            "image 260/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop331.jpg: 224x224 1 0.53, 0 0.36, 3 0.10, 5 0.01, 2 0.00, 3.2ms\n",
            "image 261/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop332.jpg: 224x224 1 0.61, 0 0.36, 3 0.04, 5 0.00, 2 0.00, 3.2ms\n",
            "image 262/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop333.jpg: 224x224 1 0.57, 0 0.38, 3 0.04, 5 0.01, 4 0.00, 3.2ms\n",
            "image 263/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop334.jpg: 224x224 1 0.49, 0 0.38, 3 0.10, 5 0.03, 4 0.00, 3.1ms\n",
            "image 264/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop335.jpg: 224x224 0 0.43, 1 0.30, 3 0.22, 5 0.04, 4 0.02, 3.1ms\n",
            "image 265/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop336.jpg: 224x224 1 0.44, 0 0.42, 3 0.14, 5 0.01, 2 0.00, 3.1ms\n",
            "image 266/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop337.jpg: 224x224 0 0.46, 3 0.30, 1 0.20, 4 0.03, 5 0.01, 3.2ms\n",
            "image 267/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop338.jpg: 224x224 1 0.57, 0 0.39, 3 0.03, 5 0.00, 2 0.00, 3.2ms\n",
            "image 268/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop339.jpg: 224x224 1 0.52, 0 0.32, 3 0.16, 5 0.01, 2 0.00, 3.2ms\n",
            "image 269/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop34.jpg: 224x224 1 0.59, 0 0.38, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 270/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop340.jpg: 224x224 1 0.51, 0 0.35, 3 0.12, 5 0.02, 2 0.00, 3.2ms\n",
            "image 271/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop341.jpg: 224x224 1 0.56, 0 0.36, 3 0.06, 5 0.02, 4 0.00, 3.1ms\n",
            "image 272/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop342.jpg: 224x224 1 0.43, 0 0.36, 3 0.21, 5 0.00, 4 0.00, 3.1ms\n",
            "image 273/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop343.jpg: 224x224 1 0.57, 0 0.39, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 274/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop344.jpg: 224x224 1 0.53, 0 0.33, 3 0.14, 5 0.00, 4 0.00, 3.1ms\n",
            "image 275/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop345.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 276/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop346.jpg: 224x224 1 0.62, 0 0.35, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 277/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop347.jpg: 224x224 1 0.46, 0 0.46, 3 0.07, 5 0.00, 2 0.00, 3.8ms\n",
            "image 278/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop348.jpg: 224x224 0 0.42, 1 0.40, 3 0.16, 5 0.02, 4 0.01, 3.1ms\n",
            "image 279/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop349.jpg: 224x224 1 0.53, 0 0.37, 3 0.08, 5 0.02, 2 0.00, 3.1ms\n",
            "image 280/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop35.jpg: 224x224 1 0.56, 0 0.38, 3 0.05, 5 0.01, 2 0.00, 3.2ms\n",
            "image 281/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop350.jpg: 224x224 1 0.56, 0 0.40, 3 0.05, 5 0.00, 2 0.00, 3.2ms\n",
            "image 282/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop351.jpg: 224x224 1 0.57, 0 0.37, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 283/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop352.jpg: 224x224 1 0.65, 0 0.33, 3 0.01, 5 0.00, 4 0.00, 3.1ms\n",
            "image 284/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop353.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 285/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop354.jpg: 224x224 0 0.48, 1 0.32, 3 0.14, 5 0.06, 4 0.00, 3.2ms\n",
            "image 286/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop355.jpg: 224x224 1 0.55, 0 0.38, 3 0.05, 5 0.02, 4 0.00, 3.2ms\n",
            "image 287/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop356.jpg: 224x224 1 0.54, 0 0.38, 3 0.07, 5 0.01, 2 0.00, 3.2ms\n",
            "image 288/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop357.jpg: 224x224 1 0.63, 0 0.34, 5 0.02, 3 0.01, 4 0.00, 3.6ms\n",
            "image 289/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop358.jpg: 224x224 1 0.63, 0 0.33, 3 0.04, 5 0.00, 2 0.00, 3.5ms\n",
            "image 290/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop359.jpg: 224x224 5 0.63, 0 0.28, 1 0.05, 3 0.04, 4 0.00, 3.5ms\n",
            "image 291/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop36.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 292/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop360.jpg: 224x224 1 0.60, 0 0.35, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 293/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop361.jpg: 224x224 1 0.61, 0 0.38, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 294/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop362.jpg: 224x224 1 0.49, 0 0.42, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 295/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop363.jpg: 224x224 1 0.62, 0 0.36, 3 0.02, 5 0.00, 4 0.00, 3.2ms\n",
            "image 296/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop364.jpg: 224x224 0 0.46, 5 0.23, 1 0.17, 3 0.15, 4 0.00, 3.1ms\n",
            "image 297/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop365.jpg: 224x224 1 0.55, 0 0.38, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 298/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop366.jpg: 224x224 3 0.34, 0 0.34, 1 0.31, 5 0.01, 2 0.00, 3.1ms\n",
            "image 299/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop367.jpg: 224x224 0 0.46, 1 0.44, 3 0.08, 5 0.02, 2 0.00, 3.1ms\n",
            "image 300/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop368.jpg: 224x224 1 0.48, 0 0.45, 3 0.06, 5 0.01, 2 0.00, 3.8ms\n",
            "image 301/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop369.jpg: 224x224 1 0.53, 0 0.37, 3 0.08, 5 0.02, 2 0.00, 3.1ms\n",
            "image 302/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop37.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 303/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop370.jpg: 224x224 0 0.42, 1 0.29, 5 0.21, 3 0.07, 4 0.00, 3.1ms\n",
            "image 304/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop371.jpg: 224x224 1 0.55, 0 0.40, 3 0.05, 5 0.01, 2 0.00, 3.2ms\n",
            "image 305/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop372.jpg: 224x224 1 0.53, 0 0.38, 3 0.09, 5 0.00, 2 0.00, 3.2ms\n",
            "image 306/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop373.jpg: 224x224 5 0.45, 0 0.35, 1 0.14, 3 0.06, 4 0.00, 3.2ms\n",
            "image 307/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop374.jpg: 224x224 0 0.39, 1 0.34, 3 0.20, 5 0.07, 2 0.00, 3.1ms\n",
            "image 308/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop375.jpg: 224x224 1 0.63, 0 0.36, 3 0.01, 5 0.00, 4 0.00, 3.2ms\n",
            "image 309/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop376.jpg: 224x224 1 0.51, 0 0.37, 3 0.12, 5 0.01, 2 0.00, 3.2ms\n",
            "image 310/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop377.jpg: 224x224 1 0.61, 0 0.37, 3 0.02, 5 0.00, 2 0.00, 3.2ms\n",
            "image 311/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop378.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 312/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop379.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.00, 2 0.00, 3.2ms\n",
            "image 313/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop38.jpg: 224x224 0 0.41, 1 0.33, 3 0.14, 5 0.11, 4 0.00, 3.2ms\n",
            "image 314/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop380.jpg: 224x224 1 0.52, 0 0.39, 3 0.06, 5 0.02, 4 0.00, 3.2ms\n",
            "image 315/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop381.jpg: 224x224 1 0.64, 0 0.35, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 316/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop382.jpg: 224x224 1 0.55, 0 0.38, 3 0.07, 5 0.01, 2 0.00, 3.2ms\n",
            "image 317/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop383.jpg: 224x224 1 0.58, 0 0.36, 3 0.06, 5 0.00, 4 0.00, 3.1ms\n",
            "image 318/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop384.jpg: 224x224 1 0.54, 0 0.41, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 319/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop385.jpg: 224x224 1 0.52, 0 0.38, 3 0.06, 5 0.03, 2 0.00, 3.2ms\n",
            "image 320/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop386.jpg: 224x224 1 0.64, 0 0.32, 3 0.03, 5 0.01, 2 0.00, 3.2ms\n",
            "image 321/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop387.jpg: 224x224 1 0.54, 0 0.40, 3 0.06, 5 0.01, 2 0.00, 3.2ms\n",
            "image 322/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop388.jpg: 224x224 1 0.54, 0 0.40, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 323/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop389.jpg: 224x224 1 0.66, 0 0.32, 3 0.03, 5 0.00, 2 0.00, 3.8ms\n",
            "image 324/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop39.jpg: 224x224 1 0.68, 0 0.30, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 325/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop390.jpg: 224x224 1 0.53, 0 0.37, 3 0.08, 5 0.02, 2 0.00, 3.1ms\n",
            "image 326/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop391.jpg: 224x224 1 0.55, 0 0.42, 3 0.03, 5 0.00, 2 0.00, 3.2ms\n",
            "image 327/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop392.jpg: 224x224 1 0.58, 0 0.40, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 328/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop393.jpg: 224x224 0 0.48, 3 0.29, 1 0.16, 5 0.07, 4 0.01, 3.1ms\n",
            "image 329/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop394.jpg: 224x224 1 0.45, 0 0.44, 3 0.10, 5 0.01, 2 0.00, 3.1ms\n",
            "image 330/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop395.jpg: 224x224 1 0.55, 0 0.38, 3 0.07, 5 0.01, 2 0.00, 3.1ms\n",
            "image 331/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop396.jpg: 224x224 1 0.60, 0 0.36, 3 0.02, 5 0.02, 4 0.00, 3.1ms\n",
            "image 332/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop397.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.2ms\n",
            "image 333/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop398.jpg: 224x224 1 0.54, 0 0.35, 3 0.09, 5 0.01, 2 0.00, 3.1ms\n",
            "image 334/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop399.jpg: 224x224 1 0.32, 0 0.30, 3 0.25, 5 0.10, 4 0.01, 3.1ms\n",
            "image 335/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop4.jpg: 224x224 1 0.58, 0 0.39, 3 0.03, 5 0.00, 4 0.00, 3.2ms\n",
            "image 336/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop40.jpg: 224x224 1 0.54, 0 0.35, 3 0.11, 5 0.01, 2 0.00, 3.2ms\n",
            "image 337/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop400.jpg: 224x224 1 0.60, 0 0.35, 3 0.04, 5 0.01, 2 0.00, 3.2ms\n",
            "image 338/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop401.jpg: 224x224 1 0.53, 0 0.39, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 339/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop402.jpg: 224x224 0 0.44, 1 0.27, 5 0.19, 3 0.10, 4 0.00, 3.2ms\n",
            "image 340/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop403.jpg: 224x224 5 0.44, 0 0.34, 1 0.15, 3 0.07, 4 0.00, 3.6ms\n",
            "image 341/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop404.jpg: 224x224 1 0.52, 0 0.36, 3 0.08, 5 0.03, 2 0.00, 3.2ms\n",
            "image 342/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop405.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 2 0.00, 3.2ms\n",
            "image 343/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop406.jpg: 224x224 1 0.53, 0 0.39, 3 0.09, 5 0.00, 2 0.00, 3.2ms\n",
            "image 344/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop407.jpg: 224x224 1 0.53, 0 0.39, 3 0.07, 5 0.02, 4 0.00, 3.2ms\n",
            "image 345/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop408.jpg: 224x224 0 0.44, 3 0.24, 1 0.19, 5 0.13, 4 0.00, 3.3ms\n",
            "image 346/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop409.jpg: 224x224 1 0.58, 0 0.37, 3 0.05, 5 0.00, 4 0.00, 3.9ms\n",
            "image 347/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop41.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 348/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop410.jpg: 224x224 1 0.53, 0 0.44, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 349/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop411.jpg: 224x224 1 0.54, 0 0.38, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 350/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop412.jpg: 224x224 1 0.53, 0 0.39, 3 0.06, 5 0.02, 2 0.00, 3.1ms\n",
            "image 351/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop413.jpg: 224x224 1 0.58, 0 0.37, 5 0.03, 3 0.02, 4 0.00, 3.2ms\n",
            "image 352/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop414.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 353/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop415.jpg: 224x224 0 0.62, 1 0.34, 3 0.03, 5 0.01, 2 0.00, 3.1ms\n",
            "image 354/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop416.jpg: 224x224 1 0.51, 0 0.40, 3 0.07, 5 0.02, 2 0.00, 3.2ms\n",
            "image 355/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop417.jpg: 224x224 1 0.55, 0 0.38, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 356/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop418.jpg: 224x224 1 0.57, 0 0.37, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 357/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop419.jpg: 224x224 1 0.55, 0 0.36, 3 0.07, 5 0.02, 2 0.00, 3.2ms\n",
            "image 358/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop42.jpg: 224x224 0 0.44, 1 0.34, 3 0.12, 5 0.10, 4 0.00, 3.1ms\n",
            "image 359/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop420.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 360/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop421.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.01, 2 0.00, 3.2ms\n",
            "image 361/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop422.jpg: 224x224 0 0.41, 3 0.27, 1 0.27, 5 0.05, 2 0.00, 3.1ms\n",
            "image 362/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop423.jpg: 224x224 1 0.55, 0 0.40, 3 0.03, 5 0.02, 4 0.00, 3.1ms\n",
            "image 363/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop424.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 364/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop425.jpg: 224x224 1 0.55, 0 0.38, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 365/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop426.jpg: 224x224 1 0.56, 0 0.37, 3 0.07, 5 0.00, 2 0.00, 3.2ms\n",
            "image 366/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop427.jpg: 224x224 1 0.46, 0 0.36, 5 0.13, 3 0.04, 4 0.00, 3.2ms\n",
            "image 367/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop428.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.2ms\n",
            "image 368/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop429.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 369/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop43.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.00, 2 0.00, 3.9ms\n",
            "image 370/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop430.jpg: 224x224 1 0.55, 0 0.39, 3 0.04, 5 0.02, 4 0.00, 3.1ms\n",
            "image 371/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop431.jpg: 224x224 1 0.55, 0 0.39, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 372/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop432.jpg: 224x224 1 0.46, 0 0.43, 3 0.10, 5 0.01, 2 0.00, 3.1ms\n",
            "image 373/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop433.jpg: 224x224 1 0.60, 0 0.38, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 374/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop434.jpg: 224x224 1 0.51, 0 0.40, 3 0.08, 5 0.00, 2 0.00, 3.2ms\n",
            "image 375/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop435.jpg: 224x224 1 0.49, 0 0.37, 3 0.13, 5 0.01, 4 0.00, 3.2ms\n",
            "image 376/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop436.jpg: 224x224 0 0.47, 1 0.32, 3 0.16, 5 0.04, 4 0.00, 3.2ms\n",
            "image 377/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop437.jpg: 224x224 1 0.50, 0 0.35, 3 0.12, 5 0.03, 2 0.00, 3.2ms\n",
            "image 378/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop438.jpg: 224x224 0 0.39, 1 0.31, 5 0.21, 3 0.09, 4 0.00, 3.2ms\n",
            "image 379/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop439.jpg: 224x224 0 0.42, 5 0.25, 1 0.24, 3 0.09, 4 0.00, 3.2ms\n",
            "image 380/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop44.jpg: 224x224 1 0.55, 0 0.40, 3 0.03, 5 0.02, 4 0.00, 3.2ms\n",
            "image 381/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop440.jpg: 224x224 1 0.58, 0 0.37, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 382/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop441.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 383/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop442.jpg: 224x224 1 0.56, 0 0.40, 3 0.03, 5 0.00, 4 0.00, 3.2ms\n",
            "image 384/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop443.jpg: 224x224 1 0.53, 0 0.38, 3 0.08, 5 0.00, 2 0.00, 3.2ms\n",
            "image 385/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop444.jpg: 224x224 0 0.37, 1 0.26, 5 0.19, 3 0.17, 4 0.01, 3.3ms\n",
            "image 386/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop445.jpg: 224x224 1 0.47, 0 0.41, 3 0.12, 5 0.01, 2 0.00, 3.1ms\n",
            "image 387/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop446.jpg: 224x224 1 0.49, 0 0.47, 3 0.03, 5 0.01, 2 0.00, 3.2ms\n",
            "image 388/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop447.jpg: 224x224 1 0.56, 0 0.40, 3 0.02, 5 0.02, 4 0.00, 3.5ms\n",
            "image 389/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop448.jpg: 224x224 1 0.38, 0 0.33, 3 0.27, 5 0.01, 2 0.00, 3.5ms\n",
            "image 390/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop449.jpg: 224x224 1 0.53, 0 0.42, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 391/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop45.jpg: 224x224 1 0.64, 0 0.33, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 392/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop450.jpg: 224x224 1 0.42, 0 0.41, 3 0.12, 5 0.05, 4 0.00, 3.8ms\n",
            "image 393/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop451.jpg: 224x224 1 0.56, 0 0.40, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 394/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop452.jpg: 224x224 1 0.57, 0 0.38, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 395/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop453.jpg: 224x224 1 0.61, 0 0.34, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 396/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop454.jpg: 224x224 1 0.44, 0 0.40, 3 0.15, 5 0.01, 2 0.00, 3.1ms\n",
            "image 397/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop455.jpg: 224x224 1 0.57, 0 0.37, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 398/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop456.jpg: 224x224 0 0.39, 1 0.36, 3 0.20, 5 0.05, 4 0.00, 3.1ms\n",
            "image 399/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop457.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 400/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop458.jpg: 224x224 1 0.40, 0 0.36, 3 0.24, 5 0.00, 2 0.00, 3.2ms\n",
            "image 401/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop459.jpg: 224x224 1 0.57, 0 0.36, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 402/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop46.jpg: 224x224 1 0.56, 0 0.37, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 403/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop460.jpg: 224x224 1 0.58, 0 0.35, 5 0.04, 3 0.02, 4 0.00, 3.1ms\n",
            "image 404/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop461.jpg: 224x224 1 0.50, 0 0.40, 3 0.10, 5 0.01, 2 0.00, 3.1ms\n",
            "image 405/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop462.jpg: 224x224 1 0.60, 0 0.36, 5 0.02, 3 0.02, 4 0.00, 3.1ms\n",
            "image 406/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop463.jpg: 224x224 1 0.55, 0 0.38, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 407/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop464.jpg: 224x224 1 0.54, 0 0.38, 3 0.07, 5 0.01, 2 0.00, 3.1ms\n",
            "image 408/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop465.jpg: 224x224 1 0.50, 0 0.41, 3 0.09, 5 0.00, 2 0.00, 3.1ms\n",
            "image 409/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop466.jpg: 224x224 1 0.57, 0 0.38, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 410/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop467.jpg: 224x224 1 0.62, 0 0.36, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 411/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop468.jpg: 224x224 1 0.46, 0 0.43, 3 0.08, 5 0.03, 4 0.00, 3.1ms\n",
            "image 412/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop469.jpg: 224x224 1 0.57, 0 0.35, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 413/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop47.jpg: 224x224 1 0.62, 0 0.36, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 414/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop470.jpg: 224x224 1 0.57, 0 0.37, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 415/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop471.jpg: 224x224 0 0.69, 1 0.19, 3 0.12, 5 0.00, 2 0.00, 3.8ms\n",
            "image 416/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop472.jpg: 224x224 0 0.61, 5 0.24, 3 0.13, 1 0.02, 4 0.00, 3.1ms\n",
            "image 417/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop473.jpg: 224x224 1 0.52, 0 0.40, 3 0.06, 5 0.02, 4 0.00, 3.1ms\n",
            "image 418/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop474.jpg: 224x224 1 0.60, 0 0.34, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 419/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop475.jpg: 224x224 0 0.38, 1 0.35, 3 0.25, 5 0.01, 2 0.00, 3.2ms\n",
            "image 420/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop476.jpg: 224x224 1 0.60, 0 0.35, 3 0.05, 5 0.01, 2 0.00, 3.2ms\n",
            "image 421/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop477.jpg: 224x224 1 0.58, 0 0.39, 3 0.03, 5 0.00, 4 0.00, 3.2ms\n",
            "image 422/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop478.jpg: 224x224 0 0.64, 1 0.32, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 423/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop479.jpg: 224x224 1 0.50, 0 0.38, 5 0.06, 3 0.05, 4 0.00, 3.1ms\n",
            "image 424/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop48.jpg: 224x224 1 0.55, 0 0.39, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 425/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop480.jpg: 224x224 0 0.48, 1 0.21, 3 0.15, 5 0.15, 4 0.00, 3.1ms\n",
            "image 426/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop481.jpg: 224x224 1 0.53, 0 0.37, 3 0.09, 5 0.01, 2 0.00, 3.1ms\n",
            "image 427/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop482.jpg: 224x224 1 0.48, 0 0.45, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 428/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop483.jpg: 224x224 1 0.58, 0 0.40, 3 0.01, 5 0.00, 4 0.00, 3.1ms\n",
            "image 429/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop484.jpg: 224x224 1 0.62, 0 0.36, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 430/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop485.jpg: 224x224 1 0.36, 0 0.36, 3 0.15, 5 0.13, 4 0.00, 3.1ms\n",
            "image 431/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop486.jpg: 224x224 1 0.57, 0 0.37, 3 0.04, 5 0.03, 4 0.00, 3.1ms\n",
            "image 432/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop487.jpg: 224x224 1 0.52, 0 0.43, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 433/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop488.jpg: 224x224 1 0.47, 0 0.38, 3 0.12, 5 0.02, 2 0.00, 3.1ms\n",
            "image 434/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop489.jpg: 224x224 1 0.58, 0 0.34, 3 0.07, 5 0.00, 2 0.00, 3.1ms\n",
            "image 435/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop49.jpg: 224x224 1 0.55, 0 0.39, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 436/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop490.jpg: 224x224 1 0.57, 0 0.34, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 437/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop491.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 438/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop492.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.01, 4 0.00, 3.8ms\n",
            "image 439/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop493.jpg: 224x224 1 0.40, 0 0.39, 5 0.14, 3 0.07, 4 0.00, 3.1ms\n",
            "image 440/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop494.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 441/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop495.jpg: 224x224 1 0.60, 0 0.37, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 442/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop496.jpg: 224x224 0 0.47, 1 0.31, 3 0.12, 5 0.07, 4 0.02, 3.1ms\n",
            "image 443/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop497.jpg: 224x224 1 0.60, 0 0.34, 5 0.04, 3 0.03, 4 0.00, 3.1ms\n",
            "image 444/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop498.jpg: 224x224 1 0.53, 0 0.38, 3 0.06, 5 0.03, 2 0.00, 3.1ms\n",
            "image 445/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop499.jpg: 224x224 1 0.57, 0 0.40, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 446/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop5.jpg: 224x224 1 0.60, 0 0.36, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 447/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop50.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 448/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop500.jpg: 224x224 1 0.55, 0 0.39, 5 0.03, 3 0.03, 4 0.00, 3.1ms\n",
            "image 449/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop501.jpg: 224x224 1 0.71, 0 0.28, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 450/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop502.jpg: 224x224 1 0.56, 0 0.35, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 451/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop503.jpg: 224x224 1 0.54, 0 0.38, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 452/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop504.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 453/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop505.jpg: 224x224 0 0.50, 1 0.37, 3 0.11, 5 0.01, 2 0.00, 3.1ms\n",
            "image 454/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop506.jpg: 224x224 0 0.48, 1 0.41, 3 0.10, 5 0.01, 2 0.00, 3.1ms\n",
            "image 455/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop507.jpg: 224x224 5 0.48, 0 0.30, 3 0.12, 1 0.10, 4 0.00, 3.1ms\n",
            "image 456/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop508.jpg: 224x224 1 0.59, 0 0.38, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 457/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop509.jpg: 224x224 1 0.59, 0 0.34, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 458/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop51.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 459/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop510.jpg: 224x224 1 0.60, 0 0.34, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 460/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop511.jpg: 224x224 1 0.53, 0 0.36, 3 0.11, 5 0.00, 2 0.00, 3.1ms\n",
            "image 461/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop512.jpg: 224x224 1 0.50, 0 0.40, 3 0.08, 5 0.02, 2 0.00, 3.8ms\n",
            "image 462/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop513.jpg: 224x224 0 0.34, 5 0.34, 3 0.18, 1 0.13, 4 0.01, 3.1ms\n",
            "image 463/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop514.jpg: 224x224 1 0.55, 0 0.41, 3 0.04, 5 0.00, 2 0.00, 3.4ms\n",
            "image 464/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop515.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 465/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop516.jpg: 224x224 1 0.55, 0 0.38, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 466/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop517.jpg: 224x224 0 0.47, 1 0.33, 3 0.15, 5 0.05, 4 0.01, 3.1ms\n",
            "image 467/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop518.jpg: 224x224 1 0.62, 0 0.36, 5 0.01, 3 0.01, 4 0.00, 3.1ms\n",
            "image 468/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop519.jpg: 224x224 0 0.41, 1 0.31, 3 0.23, 5 0.05, 4 0.01, 3.1ms\n",
            "image 469/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop52.jpg: 224x224 1 0.62, 0 0.35, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 470/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop520.jpg: 224x224 1 0.57, 0 0.35, 5 0.05, 3 0.04, 4 0.00, 3.1ms\n",
            "image 471/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop521.jpg: 224x224 0 0.52, 1 0.39, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 472/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop522.jpg: 224x224 1 0.50, 0 0.38, 3 0.11, 5 0.01, 2 0.00, 3.1ms\n",
            "image 473/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop523.jpg: 224x224 1 0.54, 0 0.40, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 474/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop524.jpg: 224x224 1 0.55, 0 0.37, 3 0.07, 5 0.01, 4 0.00, 3.1ms\n",
            "image 475/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop525.jpg: 224x224 0 0.43, 1 0.43, 3 0.11, 5 0.03, 4 0.00, 3.1ms\n",
            "image 476/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop526.jpg: 224x224 1 0.48, 0 0.40, 3 0.08, 5 0.02, 4 0.01, 3.1ms\n",
            "image 477/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop527.jpg: 224x224 1 0.62, 0 0.35, 3 0.01, 5 0.01, 4 0.00, 3.1ms\n",
            "image 478/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop528.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 479/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop529.jpg: 224x224 1 0.61, 0 0.34, 3 0.04, 5 0.01, 2 0.00, 3.2ms\n",
            "image 480/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop53.jpg: 224x224 1 0.54, 0 0.42, 3 0.04, 5 0.00, 4 0.00, 3.2ms\n",
            "image 481/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop530.jpg: 224x224 1 0.62, 0 0.31, 3 0.05, 5 0.02, 4 0.00, 3.2ms\n",
            "image 482/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop531.jpg: 224x224 1 0.39, 0 0.37, 5 0.16, 3 0.08, 4 0.00, 3.2ms\n",
            "image 483/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop532.jpg: 224x224 0 0.39, 1 0.34, 3 0.26, 5 0.01, 2 0.00, 3.1ms\n",
            "image 484/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop533.jpg: 224x224 1 0.62, 0 0.36, 3 0.01, 5 0.00, 4 0.00, 3.9ms\n",
            "image 485/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop534.jpg: 224x224 5 0.41, 0 0.40, 3 0.10, 1 0.09, 4 0.00, 3.1ms\n",
            "image 486/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop535.jpg: 224x224 1 0.53, 0 0.40, 3 0.06, 5 0.01, 4 0.00, 3.1ms\n",
            "image 487/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop536.jpg: 224x224 1 0.52, 0 0.32, 3 0.16, 5 0.00, 2 0.00, 3.1ms\n",
            "image 488/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop537.jpg: 224x224 1 0.51, 0 0.37, 3 0.12, 5 0.00, 2 0.00, 3.1ms\n",
            "image 489/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop538.jpg: 224x224 1 0.52, 0 0.38, 3 0.07, 5 0.03, 4 0.00, 3.1ms\n",
            "image 490/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop539.jpg: 224x224 1 0.59, 0 0.36, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 491/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop54.jpg: 224x224 1 0.52, 0 0.40, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 492/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop540.jpg: 224x224 0 0.43, 3 0.28, 1 0.25, 5 0.04, 4 0.01, 3.1ms\n",
            "image 493/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop541.jpg: 224x224 0 0.43, 1 0.29, 5 0.17, 3 0.11, 4 0.00, 3.2ms\n",
            "image 494/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop542.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 495/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop543.jpg: 224x224 0 0.39, 1 0.34, 3 0.16, 5 0.12, 2 0.00, 3.1ms\n",
            "image 496/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop544.jpg: 224x224 1 0.42, 0 0.37, 3 0.19, 5 0.03, 2 0.00, 3.1ms\n",
            "image 497/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop545.jpg: 224x224 1 0.54, 0 0.35, 3 0.11, 5 0.00, 2 0.00, 3.4ms\n",
            "image 498/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop546.jpg: 224x224 1 0.56, 0 0.38, 3 0.03, 5 0.02, 4 0.00, 3.4ms\n",
            "image 499/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop547.jpg: 224x224 1 0.54, 0 0.43, 3 0.02, 5 0.01, 4 0.00, 3.3ms\n",
            "image 500/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop548.jpg: 224x224 1 0.52, 0 0.42, 3 0.06, 5 0.00, 2 0.00, 3.2ms\n",
            "image 501/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop549.jpg: 224x224 1 0.50, 0 0.38, 3 0.11, 5 0.01, 4 0.00, 3.2ms\n",
            "image 502/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop55.jpg: 224x224 1 0.48, 0 0.45, 3 0.06, 5 0.00, 4 0.00, 3.1ms\n",
            "image 503/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop550.jpg: 224x224 1 0.63, 0 0.34, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 504/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop551.jpg: 224x224 1 0.58, 0 0.39, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 505/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop552.jpg: 224x224 1 0.54, 0 0.41, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 506/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop553.jpg: 224x224 1 0.45, 0 0.39, 3 0.11, 5 0.04, 4 0.00, 3.1ms\n",
            "image 507/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop554.jpg: 224x224 1 0.56, 0 0.33, 3 0.10, 5 0.01, 4 0.00, 3.8ms\n",
            "image 508/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop555.jpg: 224x224 1 0.65, 0 0.32, 5 0.02, 3 0.01, 4 0.00, 3.1ms\n",
            "image 509/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop556.jpg: 224x224 1 0.50, 0 0.38, 3 0.09, 5 0.03, 2 0.00, 3.1ms\n",
            "image 510/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop557.jpg: 224x224 1 0.60, 0 0.36, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 511/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop558.jpg: 224x224 1 0.63, 0 0.34, 5 0.02, 3 0.01, 4 0.00, 3.1ms\n",
            "image 512/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop559.jpg: 224x224 1 0.57, 0 0.36, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 513/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop56.jpg: 224x224 1 0.40, 0 0.39, 5 0.13, 3 0.08, 4 0.00, 3.1ms\n",
            "image 514/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop560.jpg: 224x224 1 0.54, 0 0.36, 3 0.06, 5 0.03, 4 0.00, 3.1ms\n",
            "image 515/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop561.jpg: 224x224 1 0.53, 0 0.41, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 516/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop562.jpg: 224x224 1 0.47, 0 0.36, 3 0.10, 5 0.07, 2 0.00, 3.1ms\n",
            "image 517/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop563.jpg: 224x224 1 0.49, 0 0.36, 3 0.09, 5 0.05, 4 0.01, 3.1ms\n",
            "image 518/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop564.jpg: 224x224 1 0.48, 0 0.36, 3 0.15, 5 0.01, 2 0.00, 3.1ms\n",
            "image 519/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop565.jpg: 224x224 1 0.57, 0 0.35, 3 0.05, 5 0.03, 4 0.00, 3.2ms\n",
            "image 520/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop566.jpg: 224x224 1 0.56, 0 0.35, 3 0.08, 5 0.01, 4 0.00, 3.1ms\n",
            "image 521/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop567.jpg: 224x224 0 0.38, 1 0.34, 3 0.28, 5 0.00, 4 0.00, 3.2ms\n",
            "image 522/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop568.jpg: 224x224 1 0.59, 0 0.37, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 523/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop569.jpg: 224x224 1 0.49, 0 0.37, 3 0.13, 5 0.01, 2 0.00, 3.1ms\n",
            "image 524/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop57.jpg: 224x224 1 0.53, 0 0.38, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 525/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop570.jpg: 224x224 1 0.58, 0 0.38, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 526/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop571.jpg: 224x224 1 0.57, 0 0.39, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 527/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop572.jpg: 224x224 1 0.54, 0 0.40, 3 0.06, 5 0.01, 2 0.00, 3.4ms\n",
            "image 528/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop573.jpg: 224x224 1 0.55, 0 0.36, 3 0.07, 5 0.02, 4 0.00, 3.1ms\n",
            "image 529/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop574.jpg: 224x224 1 0.54, 0 0.37, 3 0.07, 5 0.02, 4 0.00, 3.1ms\n",
            "image 530/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop575.jpg: 224x224 1 0.56, 0 0.38, 3 0.04, 5 0.02, 4 0.00, 3.8ms\n",
            "image 531/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop576.jpg: 224x224 1 0.51, 0 0.41, 3 0.05, 5 0.03, 2 0.00, 3.1ms\n",
            "image 532/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop577.jpg: 224x224 1 0.59, 0 0.35, 3 0.04, 5 0.02, 4 0.00, 3.1ms\n",
            "image 533/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop578.jpg: 224x224 1 0.48, 0 0.47, 3 0.03, 5 0.02, 4 0.00, 3.2ms\n",
            "image 534/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop579.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 535/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop58.jpg: 224x224 1 0.58, 0 0.38, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 536/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop580.jpg: 224x224 1 0.52, 0 0.37, 3 0.09, 5 0.02, 2 0.00, 3.1ms\n",
            "image 537/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop581.jpg: 224x224 0 0.38, 1 0.38, 3 0.21, 5 0.02, 4 0.00, 3.1ms\n",
            "image 538/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop582.jpg: 224x224 1 0.45, 0 0.45, 3 0.07, 5 0.03, 2 0.00, 3.1ms\n",
            "image 539/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop583.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 540/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop584.jpg: 224x224 1 0.51, 0 0.40, 3 0.07, 5 0.01, 4 0.00, 3.1ms\n",
            "image 541/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop585.jpg: 224x224 1 0.55, 0 0.42, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 542/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop586.jpg: 224x224 1 0.58, 0 0.36, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 543/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop587.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 544/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop588.jpg: 224x224 1 0.51, 0 0.33, 3 0.15, 5 0.01, 2 0.00, 3.1ms\n",
            "image 545/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop589.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 546/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop59.jpg: 224x224 1 0.59, 0 0.40, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 547/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop590.jpg: 224x224 1 0.61, 0 0.35, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 548/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop591.jpg: 224x224 0 0.50, 1 0.46, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 549/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop592.jpg: 224x224 1 0.57, 0 0.35, 3 0.07, 5 0.01, 2 0.00, 3.1ms\n",
            "image 550/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop593.jpg: 224x224 1 0.57, 0 0.34, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 551/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop594.jpg: 224x224 1 0.59, 0 0.36, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "image 552/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop595.jpg: 224x224 1 0.52, 0 0.40, 3 0.08, 5 0.00, 2 0.00, 3.1ms\n",
            "image 553/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop596.jpg: 224x224 0 0.41, 1 0.31, 3 0.28, 5 0.00, 4 0.00, 3.8ms\n",
            "image 554/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop597.jpg: 224x224 1 0.62, 0 0.35, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 555/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop598.jpg: 224x224 0 0.42, 1 0.30, 3 0.23, 5 0.04, 4 0.01, 3.1ms\n",
            "image 556/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop599.jpg: 224x224 1 0.56, 0 0.41, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 557/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop6.jpg: 224x224 1 0.54, 0 0.42, 3 0.04, 5 0.01, 2 0.00, 3.5ms\n",
            "image 558/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop60.jpg: 224x224 1 0.53, 0 0.42, 3 0.04, 5 0.01, 2 0.00, 3.2ms\n",
            "image 559/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop600.jpg: 224x224 1 0.55, 0 0.40, 3 0.03, 5 0.02, 4 0.00, 3.1ms\n",
            "image 560/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop601.jpg: 224x224 0 0.49, 1 0.46, 3 0.04, 5 0.01, 2 0.00, 3.1ms\n",
            "image 561/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop61.jpg: 224x224 1 0.54, 0 0.41, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 562/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop62.jpg: 224x224 1 0.50, 0 0.42, 3 0.05, 5 0.03, 4 0.00, 3.1ms\n",
            "image 563/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop63.jpg: 224x224 1 0.65, 0 0.30, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 564/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop64.jpg: 224x224 1 0.61, 0 0.37, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 565/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop65.jpg: 224x224 1 0.64, 0 0.34, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 566/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop66.jpg: 224x224 1 0.64, 0 0.35, 3 0.02, 5 0.00, 2 0.00, 3.1ms\n",
            "image 567/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop67.jpg: 224x224 1 0.59, 0 0.36, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 568/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop68.jpg: 224x224 1 0.55, 0 0.38, 3 0.07, 5 0.01, 4 0.00, 3.1ms\n",
            "image 569/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop69.jpg: 224x224 1 0.53, 0 0.36, 3 0.11, 5 0.00, 2 0.00, 3.1ms\n",
            "image 570/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop7.jpg: 224x224 1 0.52, 0 0.41, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 571/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop70.jpg: 224x224 1 0.61, 0 0.36, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 572/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop71.jpg: 224x224 1 0.65, 0 0.29, 3 0.06, 5 0.00, 2 0.00, 3.1ms\n",
            "image 573/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop72.jpg: 224x224 1 0.54, 0 0.40, 3 0.05, 5 0.01, 4 0.00, 3.1ms\n",
            "image 574/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop73.jpg: 224x224 1 0.52, 0 0.43, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 575/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop74.jpg: 224x224 1 0.52, 0 0.39, 3 0.07, 5 0.01, 4 0.00, 3.1ms\n",
            "image 576/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop75.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.00, 2 0.00, 3.8ms\n",
            "image 577/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop76.jpg: 224x224 1 0.52, 0 0.39, 3 0.08, 5 0.01, 2 0.00, 3.1ms\n",
            "image 578/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop77.jpg: 224x224 1 0.50, 0 0.35, 3 0.15, 5 0.00, 2 0.00, 3.1ms\n",
            "image 579/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop78.jpg: 224x224 1 0.60, 0 0.39, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 580/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop79.jpg: 224x224 0 0.44, 1 0.26, 3 0.23, 5 0.04, 4 0.03, 3.1ms\n",
            "image 581/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop8.jpg: 224x224 1 0.59, 0 0.38, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 582/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop80.jpg: 224x224 1 0.52, 0 0.45, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 583/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop81.jpg: 224x224 1 0.54, 0 0.36, 3 0.10, 5 0.00, 2 0.00, 3.4ms\n",
            "image 584/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop82.jpg: 224x224 1 0.59, 0 0.38, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 585/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop83.jpg: 224x224 1 0.56, 0 0.40, 3 0.04, 5 0.00, 2 0.00, 3.1ms\n",
            "image 586/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop84.jpg: 224x224 1 0.55, 0 0.39, 3 0.06, 5 0.01, 2 0.00, 3.1ms\n",
            "image 587/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop85.jpg: 224x224 1 0.57, 0 0.37, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 588/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop86.jpg: 224x224 1 0.56, 0 0.39, 3 0.05, 5 0.01, 2 0.00, 3.1ms\n",
            "image 589/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop87.jpg: 224x224 1 0.51, 0 0.35, 3 0.14, 5 0.00, 4 0.00, 3.1ms\n",
            "image 590/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop88.jpg: 224x224 1 0.54, 0 0.41, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 591/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop89.jpg: 224x224 1 0.40, 0 0.33, 3 0.26, 5 0.01, 2 0.00, 3.6ms\n",
            "image 592/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop9.jpg: 224x224 1 0.60, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.3ms\n",
            "image 593/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop90.jpg: 224x224 1 0.59, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.3ms\n",
            "image 594/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop91.jpg: 224x224 1 0.53, 0 0.39, 3 0.04, 5 0.03, 4 0.00, 3.1ms\n",
            "image 595/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop92.jpg: 224x224 1 0.60, 0 0.37, 3 0.03, 5 0.00, 2 0.00, 3.1ms\n",
            "image 596/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop93.jpg: 224x224 1 0.60, 0 0.40, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 597/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop94.jpg: 224x224 1 0.64, 0 0.35, 3 0.01, 5 0.00, 2 0.00, 3.1ms\n",
            "image 598/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop95.jpg: 224x224 1 0.55, 0 0.40, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 599/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop96.jpg: 224x224 1 0.41, 0 0.38, 3 0.19, 5 0.02, 2 0.00, 3.8ms\n",
            "image 600/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop97.jpg: 224x224 1 0.54, 0 0.41, 3 0.04, 5 0.01, 4 0.00, 3.1ms\n",
            "image 601/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop98.jpg: 224x224 1 0.52, 0 0.42, 3 0.05, 5 0.00, 2 0.00, 3.1ms\n",
            "image 602/620 /content/our_cropped2/W5_20220509_4_ST220184_A_JPG.rf.e4aa48e345f56ab1a719a223e7e08da4_crop99.jpg: 224x224 1 0.58, 0 0.39, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 603/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop0.jpg: 224x224 1 0.53, 0 0.43, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 604/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop1.jpg: 224x224 1 0.49, 0 0.44, 3 0.06, 5 0.00, 4 0.00, 3.1ms\n",
            "image 605/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop10.jpg: 224x224 1 0.57, 0 0.41, 3 0.02, 5 0.00, 4 0.00, 3.1ms\n",
            "image 606/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop11.jpg: 224x224 1 0.51, 0 0.38, 3 0.10, 5 0.00, 4 0.00, 3.1ms\n",
            "image 607/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop12.jpg: 224x224 1 0.47, 0 0.43, 3 0.07, 5 0.02, 4 0.00, 3.1ms\n",
            "image 608/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop13.jpg: 224x224 1 0.54, 0 0.41, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 609/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop14.jpg: 224x224 0 0.48, 3 0.37, 5 0.11, 1 0.04, 4 0.00, 3.2ms\n",
            "image 610/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop15.jpg: 224x224 0 0.49, 1 0.46, 3 0.05, 5 0.00, 4 0.00, 3.1ms\n",
            "image 611/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop16.jpg: 224x224 1 0.55, 0 0.42, 3 0.02, 5 0.01, 4 0.00, 3.1ms\n",
            "image 612/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop17.jpg: 224x224 1 0.51, 0 0.42, 3 0.06, 5 0.02, 4 0.00, 3.1ms\n",
            "image 613/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop2.jpg: 224x224 1 0.57, 0 0.39, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 614/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop3.jpg: 224x224 1 0.56, 0 0.41, 3 0.03, 5 0.00, 4 0.00, 3.1ms\n",
            "image 615/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop4.jpg: 224x224 1 0.52, 0 0.41, 3 0.06, 5 0.01, 4 0.00, 3.1ms\n",
            "image 616/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop5.jpg: 224x224 1 0.57, 0 0.39, 3 0.03, 5 0.01, 4 0.00, 3.1ms\n",
            "image 617/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop6.jpg: 224x224 1 0.52, 0 0.39, 3 0.05, 5 0.03, 4 0.00, 3.1ms\n",
            "image 618/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop7.jpg: 224x224 1 0.49, 0 0.40, 3 0.08, 5 0.02, 4 0.00, 3.1ms\n",
            "image 619/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop8.jpg: 224x224 1 0.48, 0 0.41, 3 0.10, 5 0.01, 4 0.00, 3.1ms\n",
            "image 620/620 /content/our_cropped2/W5_20230821_3_ST230743_B_JPG.rf.78954bd80d535b8aed00173c5773ed72_crop9.jpg: 224x224 1 0.54, 0 0.41, 3 0.04, 5 0.00, 4 0.00, 3.1ms\n",
            "Speed: 2.0ms preprocess, 3.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "620 labels saved to runs/classify/predict/labels\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!yolo task=classify save_txt=True mode=predict model=\"/content/best_classifer_finetune_num2.pt\" source={\"/content/our_cropped2\"}  imgsz=224\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021a27d4",
      "metadata": {
        "id": "021a27d4",
        "outputId": "02780f26-5080-493e-abb5-32fd635310e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for glob\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b13f46",
      "metadata": {
        "id": "35b13f46",
        "outputId": "957a9ac5-b62f-4340-b422-b9bcdaf15cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ascii\n"
          ]
        }
      ],
      "source": [
        "import chardet\n",
        "\n",
        "with open(\"/hpc/home/ama191/runs/classify/predict4/labels/W5_20220509_2_ST220182_A_JPG.rf.5d17c8a1b9b500761e1d6006c44d8dc9_crop0.txt\", \"rb\") as f:\n",
        "    raw = f.read()\n",
        "    result = chardet.detect(raw)\n",
        "    encoding = result['encoding']\n",
        "    print(encoding)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d296dd99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d296dd99",
        "outputId": "3d1fa26d-844b-41dd-86bf-133d6f2909ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 78, 1: 531, 5: 7, 3: 4})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def count_top_predicted_classes(folder_path):\n",
        "    \"\"\"\n",
        "    Reads all prediction .txt files in the folder and counts how often\n",
        "    each class is the top (highest confidence) prediction in a file.\n",
        "\n",
        "    Returns:\n",
        "        Counter: A dictionary of class_id → count\n",
        "    \"\"\"\n",
        "    top_class_counter = Counter()\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            top_conf = -1\n",
        "            top_class = None\n",
        "\n",
        "            with open(file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 2:\n",
        "                        conf = float(parts[0])\n",
        "                        cls = int(parts[1])\n",
        "                        if conf > top_conf:\n",
        "                            top_conf = conf\n",
        "                            top_class = cls\n",
        "\n",
        "            if top_class is not None:\n",
        "                top_class_counter[top_class] += 1\n",
        "\n",
        "    return top_class_counter\n",
        "count_top_predicted_classes(\"/content/runs/classify/predict/labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b96989",
      "metadata": {
        "id": "98b96989"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EOt_wW1RchTa"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
